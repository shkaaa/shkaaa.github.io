---
layout:     post
title:     NLP Basic
subtitle:   LLM 
date:       2025-05-05
author:     SHK
header-img: img/post-bg-debug.png
catalog: true
tags: 
    - NLP
---

# NLP Basic

## 语言模型基础

### 1. Defination

A language model (LM) is classically defined as a probability distribution over sequences of tokens. Given a vocabulary of tokens $V$, a language model $p$assigns a probability (a number between 0 and 1) to each token sequence $x_1,\ldots,x_L \in V$, denoted as $p(x_1,…,x_L)$. 

### 2. Autoregressive LM

using the chain rule of probability: $p(x_{1:L})=\prod_{i=1}^{L}p(x_i|x_{1:i-1})$

### 3. Claude Elwood Shannon's theory

#### Information Entropy

Information entropy is a measure of the uncertainty of a **random variable**. The unit of information entropy is bit.
$$
H(X)=-\sum_{i=1}^{n}p_i \log_2{p_i}
$$

#### Self-information

$$
I(x)=-\log_2 p(x)
$$

#### Shannon's First Theorem**无失真信源编码定理**

在无噪声的情况下，信源输出的信息可以通过编码压缩到其信息熵的极限，并且可以无失真地恢复。

In the absence of noise, the information output by the information source can be compressed by coding to the limit of it's information entropy, and be restored without distortion. That is, the average code length L satisfies $H(x)\leq L < H(x)+1$

#### 香农第二定理（有噪信道编码定理，Shannon's Second Theorem）：

对于一个给定的有噪信道，其信道容量*C*是可以无差错传输信息的最大速率。如果信息传输速率*R*<*C*，则存在一种编码方式，使得在信道上传输信息的错误概率可以任意小；如果，则无论采用何种编码方式，错误概率都将大于零。

For a given noisy channel, its channel capacity *C* is the maximum rate at which information can be transmitted without error. If the information transmission rate *R*<*C*, there exists a coding method such that the error probability of information transmission over the channel can be arbitrarily small; if *R*>*C*, no matter what coding method is used, the error probability will be greater than zero.

#### Cross Entropy

$$
H(p,q)=-\sum_xp(x)\log q(x)
$$

**需要多少比特（nats）来编码样本x∼p，使用由模型q给出的压缩方案**

衡量了模型预测的概率分布与真实标签的概率分布之间的差异。

KL 散度也称为相对熵，用于衡量两个概率分布 $p$ 和 $q$ 之间的差异程度。
$$
D_{KL}(p||q)=H(p,q)-H(p)
$$
KL散度越大，说明两个分布差距越大。

### N-gram

**如果n太小，那么模型将无法捕获长距离的依赖关系**，下一个词将无法依赖于𝖲𝗍𝖺𝗇𝖿𝗈𝗋𝖽。然而，**如果n太大，统计上将无法得到概率的好估计**

### 特征抽取

Transformer

### Bert

BERT（Bidirectional Encoder Representations from Transformers）是谷歌提出，作为一个Word2Vec的替代者，其在NLP领域的11个方向大幅刷新了精度，可以说是近年来自残差网络最优突破性的一项技术了（Encoder Only）。论文的主要特点以下几点：

1. 使用了双向Transformer作为算法的主要框架，之前的模型是从左向右输入一个文本序列，或者将 left-to-right 和 right-to-left 的训练结合起来，实验的结果表明，双向训练的语言模型对语境的理解会比单向的语言模型更深刻；
2. 使用了Mask Language Model(MLM)和 Next Sentence Prediction(NSP) 的多任务训练目标；
3. 使用更强大的机器训练更大规模的数据，使BERT的结果达到了全新的高度，并且Google开源了BERT模型，用户可以直接使用BERT作为Word2Vec的转换矩阵并高效的将其应用到自己的任务中。

两个任务：Masked LM；Next Sentence Prediction

### LLM为什么DecoderOnly

**Encoder 在抽取序列中某一个词的特征时能够看到整个序列中所有的信息，即上文和下文同时看到**；而 **Decoder 中因为有 mask 机制的存在，使得它在编码某一个词的特征时只能看到自身和它之前的文本信息**。

首先概述几种主要的架构:

- 以BERT为代表的**encoder-only**
- 以T5和BART为代表的**encoder-decoder**
- 以GPT为代表的**decoder-only**，

**Encoder 更容易出现低 Rank 问题，是因为它在训练中接收的是完整输入，缺乏强约束去激活多样化的表达；而 Decoder 通常因为自回归训练机制，自然地被迫产生高表达多样性，从而缓解了低 Rank 的风险。**

decoder-only支持一直复用KV-Cache，对多轮对话更友好，因为每个Token的表示之和它之前的输入有关，而encoder-decoder和PrefixLM就难以做到。

### Adam算法

TODO

### 激活函数

不同的激活函数，特点和作用不同：

- `Sigmoid`和`tanh`的特点是将输出限制在`(0,1)`和`(-1,1)`之间，说明`Sigmoid`和`tanh`适合做概率值的处理，例如LSTM中的各种门；而`ReLU`就不行，因为`ReLU`无最大值限制，可能会出现很大值。
- `ReLU`适合用于深层网络的训练，而`Sigmoid`和`tanh`则不行，因为它们会出现梯度消失。

#### **残差连接和 normalize** 

解决梯度消失/爆炸

#### ReLU

- 当 `z>0` 时，ReLU 激活函数的导数恒为常数1，这就避免了 sigmoid 和 tanh 会在神经网络层数比较深的时候出现的梯度消失的问题
- 计算复杂度低，不再含有幂运算，只需要一个阈值就能够得到其导数；
- 经过实际实验发现，**使用 ReLU 作为激活函数，模型收敛的速度比 sigmoid 和 tanh 快**；
- 当z<0时，ReLU 激活函数的导数恒为常数0，这既带来了一些有利的方面，也导致了一些坏的方面，分别进行描述。
  - 有利的方面：在深度学习中，目标是从大量数据中学习到关键特征，也就是把密集矩阵转化为稀疏矩阵，保留数据的关键信息，去除噪音，这样的模型就有了鲁棒性。ReLU 激活函数中将 `z<0`的部分置为0，就是产生稀疏矩阵的过程。
  - 坏的方面：将 `z<0`的部分梯度直接置为0会导致 Dead ReLU Problem(神经元坏死现象)。**可能会导致部分神经元不再对输入数据做响应，无论输入什么数据，该部分神经元的参数都不会被更新**。（这个问题是一个非常严重的问题，后续不少工作都是在解决这个问题）
- ReLU 有可能会导致梯度爆炸问题，解决方法是梯度截断；

### Prefix LM / Causal LM

前缀语言模型;因果语言模型.

![img](https://wdndev.github.io/llm_interview_note/01.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/1.llm%E6%A6%82%E5%BF%B5/image/image_ZPQiHay1ZD.png)

### 大模型LLM的 训练目标

预测下一个目标

Input: "The capital of France is"

模型会最大化$P(\text{“Paris”} \mid \text{“The capital of France is”})$

| **阶段**              | **目标函数举例**                  | **目的**                           |
| --------------------- | --------------------------------- | ---------------------------------- |
| 1️⃣ 预训练              | 自回归/掩码语言建模损失           | 学习通用语言能力（语法+语义）      |
| 2️⃣ 指令微调            | 教模型如何完成任务（“你是助手…”） | 学习任务格式与指令执行             |
| 3️⃣ 强化学习（如 RLHF） | 优化输出对人类有用性（偏好）      | 提升人类满意度、对话逻辑、价值对齐 |

### 大模型处理长文本

先用大模型概括文本，再统一处理行不行？？？

要让大模型处理更长的文本，可以考虑以下几个方法：

1. **分块处理**：将长文本分割成较短的片段，然后逐个片段输入模型进行处理。这样可以避免长文本对模型内存和计算资源的压力。在处理分块文本时，可以使用重叠的方式，即将相邻片段的一部分重叠，以保持上下文的连贯性。
2. **层次建模**：通过引入层次结构，将长文本划分为更小的单元。例如，可以将文本分为段落、句子或子句等层次，然后逐层输入模型进行处理。这样可以减少每个单元的长度，提高模型处理长文本的能力。
3. **部分生成**：如果只需要模型生成文本的一部分，而不是整个文本，可以只输入部分文本作为上下文，然后让模型生成所需的部分。例如，输入前一部分文本，让模型生成后续的内容。
4. **注意力机制**：注意力机制可以帮助模型关注输入中的重要部分，可以用于处理长文本时的上下文建模。通过引入注意力机制，模型可以更好地捕捉长文本中的关键信息。
5. **模型结构优化**：通过优化模型结构和参数设置，可以提高模型处理长文本的能力。例如，可以增加模型的层数或参数量，以增加模型的表达能力。还可以使用更高效的模型架构，如Transformer等，以提高长文本的处理效率。

### 大模型涌现能力

![img](https://picx.zhimg.com/v2-95fa4f87c3b4bcd4a9507ee7d8859231_1440w.jpg)

利用In Context Learning，已经发现在各种类型的下游任务中，大语言模型都出现了涌现现象，体现在在模型规模不够大的时候，各种任务都处理不好，但是当跨过某个模型大小临界值的时候，大模型就突然能比较好地处理这些任务。

第二类具备涌现现象的技术是思维链( CoT)。CoT本质上是一种特殊的few shot prompt，就是说对于某个复杂的比如推理问题，用户把一步一步的推导过程写出来，并提供给大语言模型（如下图蓝色文字内容所示），这样大语言模型就能做一些相对复杂的推理任务。

**任务的评价指标不够平滑**：scaling law -> 涌现**Emergent Abilities**

每个子任务平滑增长，总体能力涌现

## Transformer

参数：batch size `B`, 序列长度token数`T`，模型维度Dim`D`如512，头数`H`比如8

```python
X: [B, T, D]
Q,K,V = Linear(X) -> [B, T, D]
reshape多头
Q,K,V -> [B, H ,T ,D/H ]
attention scores = Q * K.transpose(-2, -1) -> [B, H, T, T]
attention weights = softmax(scores, dim=-1) -> [B, H, T, T]对每个Query对应的K生成概率分布
```



```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SelfAttention(nn.Module):
    def __init__(self, embed_dim):
        super(SelfAttention, self).__init__()
        self.embed_dim = embed_dim
        # 定义可学习的线性变换：Q, K, V
        self.query = nn.Linear(embed_dim, embed_dim)
        self.key = nn.Linear(embed_dim, embed_dim)
        self.value = nn.Linear(embed_dim, embed_dim)
        
        self.scale = embed_dim ** 0.5  # 缩放因子

    def forward(self, x):
        """
        x: shape [batch_size, seq_len, embed_dim]
        """
        Q = self.query(x)  # [B, L, D]
        K = self.key(x)    # [B, L, D]
        V = self.value(x)  # [B, L, D]
        # 计算注意力得分（注意：K要转置）
        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale 
        # [B, L, L]
        attn_weights = F.softmax(scores, dim=-1) # [B, L, L]
        # 用注意力权重加权V
        out = torch.matmul(attn_weights, V)  # [B, L, D]

        return out, attn_weights

class MultiHeadSelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super().__init__()
        assert embed_dim % num_heads == 0, "embed_dim必须能被num_heads整除"
        
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        # 映射到多个 Q, K, V
        self.qkv = nn.Linear(embed_dim, embed_dim * 3)  # 一次性做 QKV 映射
        self.out_proj = nn.Linear(embed_dim, embed_dim)

    def forward(self, x):
        B, L, D = x.shape
        qkv = self.qkv(x)  # [B, L, 3D]
        # reshape 成 [B, L, 3, num_heads, head_dim]
        qkv = qkv.reshape(B, L, 3, self.num_heads, self.head_dim)
        # 重排
        qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, B, H, L, head_dim]
        Q, K, V = qkv[0], qkv[1], qkv[2]  # 每个是 [B, H, L, head_dim]

        # Attention: QK^T / sqrt(d_k)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)  # [B, H, L, L]
        attn = F.softmax(scores, dim=-1)  # [B, H, L, L]

        out = torch.matmul(attn, V)  # [B, H, L, head_dim]
        out = out.transpose(1, 2).reshape(B, L, D)  # 合并多头：[B, L, D]
        return self.out_proj(out)  # 最终输出形状仍是 [B, L, D]
```

#### self-attention 在计算的过程中，如何对padding位做mask？

在 Attention 机制中，同样需要忽略 padding 部分的影响，这里以transformer encoder中的self-attention为例：self-attention中，Q和K在点积之后，需要先经过mask再进行softmax，因此，**对于要屏蔽的部分，mask之后的输出需要为负无穷**，这样softmax之后输出才为0。

#### 为什么要multi-head

每个头可以学习序列中不同的关系模式，从多个视角理解输入的数据。把每个token向量降维到多个子空间。

### Flash Attention

![img](https://wdndev.github.io/llm_interview_note/02.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84/1.attention/image/image_JdHeyN9KuN.png)

FlashAttention的主要动机就是**希望把SRAM利用起来**，但是难点就在于SRAM太小了，一个普通的矩阵乘法都放不下去。FlashAttention的解决思路就是将计算模块进行分解，拆成一个个小的计算任务。

#### Softmax Tiling

##### **（1）数值稳定**

Softmax包含指数函数，所以为了避免数值溢出问题，可以将每个元素都减去最大值，再经过e^{}，最后计算结果和原来的Softmax是一致的。

##### (2) 分块计算Softmax

Softmax是按每一个Query去算的。**将输入分割成块，并在输入块上进行多次传递，从而以增量方式执行softmax缩减**。

- 不显示构造QT^K矩阵
  $$
  \mathrm{softmax}(S)V=\frac{e^S V}{\sum e^S}
  $$

  ```python
  # init 
  max_score, sum_exp, acc = -inf, 0, 0
  # 每一个key tile:
  s = q*k
  new_max  = max(max_score, max(s))
  sum_exp = e^{max_score - new_max} * sum_exp + \sum{e^{s-new_max}}
  acc = e^{max_score - new_max} * acc + \sum{e^{s-new_max}} * v
  out = acc / sum_exp
  ```

- 

![img](https://wdndev.github.io/llm_interview_note/02.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84/1.attention/image/image_bck1Jw3P5A.png)

### Batch Norm & Layer Norm

为什么Transformer用Layer Norm不用Batch Norm

BatchNorm跨batch计算均值方差会受到padding的影响。

把每一个Token的向量归一化

#### Post LN & Pre LN

pre/post 是指在FFN和MHA之前/之后 Norm

Pre Norm往往更容易训练，但最终效果不如Post Norm

#### RMSNorm

$$
y=\frac{x}{\sqrt{\frac{1}{n}\sum_{i=1}^{n}x_i^2+\epsilon}}
$$

均方根层归一化

![img](https://pic3.zhimg.com/v2-29ea6a9efb8bd55687447eb9441a834a_1440w.jpg)

### 位置编码

#### 1.1 绝对位置编码

在第$k$个向量中加入位置向量$p_k$,$p_k$只依赖于位置信号$k$

- 训练式：直接**将位置编码当作可训练参数**，比如最大长度为512，编码维度为768，那么就初始化一个512×768的矩阵作为位置向量，让它随着训练过程更新。缺点是没有**外推性**。（BERT）

- 三角式：
  $$
  \begin{cases}
  \mathbf{p}_{k,2i}=\sin(k/10000^{2i/d})\\
  \mathbf{p}_{k,2i+1}=\cos(k/10000^{2i/d})\\
  \end{cases}
  $$
  

#### 1.2 相对位置编码

在**算Attention的时候考虑当前位置与被Attention的位置的相对距离**。Rope.

假设计算m和n的attention
$$
a_{m,n}=\frac{\exp{(\frac{x_m^Tk_n}{\sqrt{d_k}})}}{\sum_{j=1}^{N} \exp{(\frac{q_m^Tk_j}{\sqrt{d_k}})}}
$$
RoPE 旨在通过一种新的方式将 **相对位置信息** 直接嵌入到 **自注意力机制的 Query 和 Key** 中，实现对 **任意长序列的强泛化能力**。
$$
\mathrm{score}_{i,j}^{rope}=(\mathbf{q}_i \cdot R(i))^T(\mathbf{k}_j \cdot R(j))
$$
Rope会对向量的每两个维度进行一次旋转
$$
\mathbf{x}=[(x_0,x_1),\ldots ,(x_{d-2},x_{d-1})]
$$
前面的维度旋转的多，后面旋转的少

![](https://shkaaa.github.io/picx-images-hosting/Screenshot-2025-05-06-at-10.37.45.3rbfx2zyen.webp)

![](https://shkaaa.github.io/picx-images-hosting/Screenshot-2025-05-06-at-12.16.56.86tv2fvwip.webp)

```python
import torch
import math

def apply_rope(x, seq_len, head_dim):
  # torch.arange(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)
    position_ids = torch.arange(seq_len, dtype=torch.float).unsqueeze(1)
    dim_ids = torch.arange(0, head_dim, 2, dtype=torch.float)
    inv_freq = 1.0 / (10000 ** (dim_ids / head_dim))
    sinusoid_inp = torch.einsum("i,j->ij", position_ids, inv_freq)
    sin = torch.sin(sinusoid_inp)
    cos = torch.cos(sinusoid_inp)

    x1 = x[..., ::2]
    x2 = x[..., 1::2]
    x_rotated = torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)
    return x_rotated
```

| **操作类型** | **einsum 写法** | **等价 PyTorch 操作**           |
| ------------ | --------------- | ------------------------------- |
| 向量点积     | "i,i->"         | torch.dot(a, b)                 |
| 矩阵乘法     | "ik,kj->ij"     | torch.matmul(A, B)              |
| 外积         | "i,j->ij"       | a.unsqueeze(1) * b.unsqueeze(0) |
| 转置         | "ij->ji"        | a.T                             |
| 批次矩阵乘法 | "bij,bjk->bik"  | torch.bmm(A, B)                 |
| 广播乘法     | "bcd,cd->bcd"   | a * b（带广播的 elementwise）   |
| 总和缩减     | "abc->"         | a.sum()                         |

## Tokenize 分词

- **word/词**，词，是最自然的语言单元。对于英文等自然语言来说，存在着天然的分隔符，如空格或一些标点符号等，对词的切分相对容易。但是对于一些东亚文字包括中文来说，就需要某种分词算法才行。顺便说一下，Tokenizers库中，基于规则切分部分，**采用了spaCy和Moses两个库**。如果基于词来做词汇表，由于**长尾现象**的存在，**这个词汇表可能会超大**。像Transformer XL库就用到了一个**26.7万**个单词的词汇表。这需要极大的embedding matrix才能存得下。embedding matrix是用于查找取用token的embedding vector的。这对于内存或者显存都是极大的挑战。常规的词汇表，**一般大小不超过5万**。
- **char/字符**，即最基本的字符，如英语中的'a','b','c'或中文中的'你'，'我'，'他'等。而一般来讲，字符的数量是**少量有限**的。这样做的问题是，由于字符数量太小，我们在为每个字符学习嵌入向量的时候，每个向量就容纳了太多的语义在内，学习起来非常困难。
- **subword/子词级**，它介于字符和单词之间。比如说'Transformers'可能会被分成'Transform'和'ers'两个部分。这个方案**平衡了词汇量和语义独立性**，是相对较优的方案。它的处理原则是，**常用词应该保持原状，生僻词应该拆分成子词以共享token压缩空间**。

### BPE(Byte-Pair Encoding)

Qwen英文 -- BPE；对于 **中文**字符：**每个汉字当作一个独立的 token 编码**（不像传统 BPE 需要拼接多个字符）。

BPE，即字节对编码。其核心思想在于将**最常出现的子词对合并，直到词汇表达到预定的大小时停止**。从前到后遍历相邻的pair，找频数最大的合并。

### WordPiece

Bert所用**子词级别的分词算法**。

> 哪一对组合后的 token 序列在预测目标词（如下一词）时使整体语言模型的概率最大

最大的
$$
\frac{P(AB)}{P(A)P(B)}
$$

1. 生成所有可能的子词组合。
2. 计算每个候选词的得分，选择得分最高的候选词加入词汇表。

### Unigram

每次从词汇表中删除词汇的**原则是使预定义的损失最小**

Unigram算法每次**会从词汇表中挑出使得loss增长最小的10%~20%的词汇**来删除。

#### Token 重复

**增强常见模式的学习**，缓解稀疏性，提高特定领域表现

过拟合，降低多样性与创造性，浪费训练资源

## 激活函数

### FFN层的计算公式

$$
\mathrm{FFN} = \mathrm{Relu}(\mathbf{xW}_1+\mathbf{b}_1)\mathbf{W}_2+\mathbf{b}_2
$$

#### Gelu

$$
\text{GELU}(x) = x \cdot \Phi(x)
$$

#### Gelu近似

$$
0.5x \left(1 + \tanh\left( \sqrt{\frac{2}{\pi}} \left(x + 0.044715x^3 \right) \right) \right)
$$



### SwiGLU

SwiGLU 是一种**门控激活机制** ，相比传统的激活函数（如 ReLU、GELU），它能提供更强的表达能力和更优的训练效果。其公式如下
$$
\text{SwiGLU}(x) = \text{GELU}(xW_g + b_g) \otimes (xW_x + b_x)
$$

$$
\text{SwiGLU}(x) = \text{SiLU}(xW_g + b_g) \otimes (xW_x + b_x)
$$

### SiLU vs ReLU vs GELU vs SwiGLU

| 激活函数     | 公式              | 是否平滑 | 是否有门控机制 | 是否常用在大模型中                  |
| ------------ | ----------------- | -------- | -------------- | ----------------------------------- |
| ReLU         | max(0,*x*)        | ❌ 不光滑 | ❌ 无           | ❌ 已较少使用                        |
| GELU         | *x*⋅Φ(*x*)        | ✅ 是     | ❌ 无           | ✅ 极其广泛                          |
| SiLU / Swish | *x*⋅*σ*(*x*)      | ✅ 是     | ✅ 有           | ✅ 小模型/高效模型中常用             |
| SwiGLU       | GELU(xW_g)⊗(xW_x) | ✅ 是     | ✅ 有           | ✅ 当前主流（LLaMA/Qwen/Mistral 等） |

GLU -- 门控机制 Swish -- 激活函数
$$
\mathrm{Swish}(x)=x\frac{1}{1+e^{-x}}
$$

## 解码

参数

```json
{
 "top_k": 10,
 "temperature": 0.95,
 "num_beams": 1,
 "top_p": 0.8,
 "repetition_penalty": 1.5,
 "max_tokens": 30000,
 "message": [
        {
 "content": "你好！",
 "role": "user"
        }
    ]
}

```

#### TOP-K采样

采样概率最高的top-k个值

#### TOP-P采样

累计概率达到阈值

#### Temperature

作用于 Softmax，调整概率分布
$$
\mathrm{Softmax}(z_i)=\frac{\exp(\frac{z_i}{T})}{\sum_j\exp(\frac{z_i}{T})}
$$

#### 联合采样

通常是将 **top-k、top-p、Temperature 联合起来使用**。使用的先后顺序是` top-k->top-p->Temperature`。

还是以前面的例子为例。

首先设置 `top-k = 3`，表示保留概率最高的3个 token。这样就会保留女孩、鞋子、大象这3个 token。

- 女孩：0.664
- 鞋子：0.199
- 大象：0.105

接下来，可以使用 top-p 的方法，保留概率的累计和达到 0.8 的单词，也就是选取女孩和鞋子这两个 token。接着使用 Temperature = 0.7 进行归一化，变成：

- 女孩：0.660
- 鞋子：0.340

## MOE模型

- 对于传统的深度学习模型，**对每一个样本都会激活整个模型**，这会导致在训练成本上，以**大约二次方的速度增长**，因为**模型大小和训练样本数目都增加了**。

对小模型增加专家？

