---
layout:     post
title:     NLP Basic
subtitle:   LLM 
date:       2025-99-99
author:     SHK
header-img: img/post-bg-debug.png
catalog: true
tags: 
    - NLP
---

# NLP Basic

## è¯­è¨€æ¨¡å‹åŸºç¡€

### 1. Defination

A language model (LM) is classically defined as a probability distribution over sequences of tokens. Given a vocabulary of tokens $V$, a language model $p$assigns a probability (a number between 0 and 1) to each token sequence $x_1,\ldots,x_L \in V$, denoted as $p(x_1,â€¦,x_L)$. 

### 2. Autoregressive LM

using the chain rule of probability: $p(x_{1:L})=\prod_{i=1}^{L}p(x_i|x_{1:i-1})$

### 3. Claude Elwood Shannon's theory

#### Information Entropy

Information entropy is a measure of the uncertainty of a **random variable**. The unit of information entropy is bit.
$$
H(X)=-\sum_{i=1}^{n}p_i \log_2{p_i}
$$

#### Self-information

$$
I(x)=-\log_2 p(x)
$$

#### Shannon's First Theorem**æ— å¤±çœŸä¿¡æºç¼–ç å®šç†**

åœ¨æ— å™ªå£°çš„æƒ…å†µä¸‹ï¼Œä¿¡æºè¾“å‡ºçš„ä¿¡æ¯å¯ä»¥é€šè¿‡ç¼–ç å‹ç¼©åˆ°å…¶ä¿¡æ¯ç†µçš„æé™ï¼Œå¹¶ä¸”å¯ä»¥æ— å¤±çœŸåœ°æ¢å¤ã€‚

In the absence of noise, the information output by the information source can be compressed by coding to the limit of it's information entropy, and be restored without distortion. That is, the average code length L satisfies $H(x)\leq L < H(x)+1$

#### é¦™å†œç¬¬äºŒå®šç†ï¼ˆæœ‰å™ªä¿¡é“ç¼–ç å®šç†ï¼ŒShannon's Second Theoremï¼‰ï¼š

å¯¹äºä¸€ä¸ªç»™å®šçš„æœ‰å™ªä¿¡é“ï¼Œå…¶ä¿¡é“å®¹é‡*C*æ˜¯å¯ä»¥æ— å·®é”™ä¼ è¾“ä¿¡æ¯çš„æœ€å¤§é€Ÿç‡ã€‚å¦‚æœä¿¡æ¯ä¼ è¾“é€Ÿç‡*R*<*C*ï¼Œåˆ™å­˜åœ¨ä¸€ç§ç¼–ç æ–¹å¼ï¼Œä½¿å¾—åœ¨ä¿¡é“ä¸Šä¼ è¾“ä¿¡æ¯çš„é”™è¯¯æ¦‚ç‡å¯ä»¥ä»»æ„å°ï¼›å¦‚æœï¼Œåˆ™æ— è®ºé‡‡ç”¨ä½•ç§ç¼–ç æ–¹å¼ï¼Œé”™è¯¯æ¦‚ç‡éƒ½å°†å¤§äºé›¶ã€‚

For a given noisy channel, its channel capacity *C* is the maximum rate at which information can be transmitted without error. If the information transmission rate *R*<*C*, there exists a coding method such that the error probability of information transmission over the channel can be arbitrarily small; if *R*>*C*, no matter what coding method is used, the error probability will be greater than zero.

#### Cross Entropy

$$
H(p,q)=-\sum_xp(x)\log q(x)
$$

**éœ€è¦å¤šå°‘æ¯”ç‰¹ï¼ˆnatsï¼‰æ¥ç¼–ç æ ·æœ¬xâˆ¼pï¼Œä½¿ç”¨ç”±æ¨¡å‹qç»™å‡ºçš„å‹ç¼©æ–¹æ¡ˆ**

è¡¡é‡äº†æ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒä¸çœŸå®æ ‡ç­¾çš„æ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚

KL æ•£åº¦ä¹Ÿç§°ä¸ºç›¸å¯¹ç†µï¼Œç”¨äºè¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒ $p$ å’Œ $q$ ä¹‹é—´çš„å·®å¼‚ç¨‹åº¦ã€‚
$$
D_{KL}(p||q)=H(p,q)-H(p)
$$
KLæ•£åº¦è¶Šå¤§ï¼Œè¯´æ˜ä¸¤ä¸ªåˆ†å¸ƒå·®è·è¶Šå¤§ã€‚

### N-gram

**å¦‚æœnå¤ªå°ï¼Œé‚£ä¹ˆæ¨¡å‹å°†æ— æ³•æ•è·é•¿è·ç¦»çš„ä¾èµ–å…³ç³»**ï¼Œä¸‹ä¸€ä¸ªè¯å°†æ— æ³•ä¾èµ–äºğ–²ğ—ğ–ºğ—‡ğ–¿ğ—ˆğ—‹ğ–½ã€‚ç„¶è€Œï¼Œ**å¦‚æœnå¤ªå¤§ï¼Œç»Ÿè®¡ä¸Šå°†æ— æ³•å¾—åˆ°æ¦‚ç‡çš„å¥½ä¼°è®¡**

### è¯å‘é‡/åˆ†è¯

### ç‰¹å¾æŠ½å–

Transformer

### Bert

BERTï¼ˆBidirectional Encoder Representations from Transformersï¼‰æ˜¯è°·æ­Œæå‡ºï¼Œä½œä¸ºä¸€ä¸ªWord2Vecçš„æ›¿ä»£è€…ï¼Œå…¶åœ¨NLPé¢†åŸŸçš„11ä¸ªæ–¹å‘å¤§å¹…åˆ·æ–°äº†ç²¾åº¦ï¼Œå¯ä»¥è¯´æ˜¯è¿‘å¹´æ¥è‡ªæ®‹å·®ç½‘ç»œæœ€ä¼˜çªç ´æ€§çš„ä¸€é¡¹æŠ€æœ¯äº†ï¼ˆEncoder Onlyï¼‰ã€‚è®ºæ–‡çš„ä¸»è¦ç‰¹ç‚¹ä»¥ä¸‹å‡ ç‚¹ï¼š

1. ä½¿ç”¨äº†åŒå‘Transformerä½œä¸ºç®—æ³•çš„ä¸»è¦æ¡†æ¶ï¼Œä¹‹å‰çš„æ¨¡å‹æ˜¯ä»å·¦å‘å³è¾“å…¥ä¸€ä¸ªæ–‡æœ¬åºåˆ—ï¼Œæˆ–è€…å°† left-to-right å’Œ right-to-left çš„è®­ç»ƒç»“åˆèµ·æ¥ï¼Œå®éªŒçš„ç»“æœè¡¨æ˜ï¼ŒåŒå‘è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å¯¹è¯­å¢ƒçš„ç†è§£ä¼šæ¯”å•å‘çš„è¯­è¨€æ¨¡å‹æ›´æ·±åˆ»ï¼›
2. ä½¿ç”¨äº†Mask Language Model(MLM)å’Œ Next Sentence Prediction(NSP) çš„å¤šä»»åŠ¡è®­ç»ƒç›®æ ‡ï¼›
3. ä½¿ç”¨æ›´å¼ºå¤§çš„æœºå™¨è®­ç»ƒæ›´å¤§è§„æ¨¡çš„æ•°æ®ï¼Œä½¿BERTçš„ç»“æœè¾¾åˆ°äº†å…¨æ–°çš„é«˜åº¦ï¼Œå¹¶ä¸”Googleå¼€æºäº†BERTæ¨¡å‹ï¼Œç”¨æˆ·å¯ä»¥ç›´æ¥ä½¿ç”¨BERTä½œä¸ºWord2Vecçš„è½¬æ¢çŸ©é˜µå¹¶é«˜æ•ˆçš„å°†å…¶åº”ç”¨åˆ°è‡ªå·±çš„ä»»åŠ¡ä¸­ã€‚

ä¸¤ä¸ªä»»åŠ¡ï¼šMasked LMï¼›Next Sentence Prediction

### LLMä¸ºä»€ä¹ˆDecoderOnly

**Encoder åœ¨æŠ½å–åºåˆ—ä¸­æŸä¸€ä¸ªè¯çš„ç‰¹å¾æ—¶èƒ½å¤Ÿçœ‹åˆ°æ•´ä¸ªåºåˆ—ä¸­æ‰€æœ‰çš„ä¿¡æ¯ï¼Œå³ä¸Šæ–‡å’Œä¸‹æ–‡åŒæ—¶çœ‹åˆ°**ï¼›è€Œ **Decoder ä¸­å› ä¸ºæœ‰ mask æœºåˆ¶çš„å­˜åœ¨ï¼Œä½¿å¾—å®ƒåœ¨ç¼–ç æŸä¸€ä¸ªè¯çš„ç‰¹å¾æ—¶åªèƒ½çœ‹åˆ°è‡ªèº«å’Œå®ƒä¹‹å‰çš„æ–‡æœ¬ä¿¡æ¯**ã€‚

é¦–å…ˆæ¦‚è¿°å‡ ç§ä¸»è¦çš„æ¶æ„:

- ä»¥BERTä¸ºä»£è¡¨çš„**encoder-only**
- ä»¥T5å’ŒBARTä¸ºä»£è¡¨çš„**encoder-decoder**
- ä»¥GPTä¸ºä»£è¡¨çš„**decoder-only**ï¼Œ

**Encoder æ›´å®¹æ˜“å‡ºç°ä½ Rank é—®é¢˜ï¼Œæ˜¯å› ä¸ºå®ƒåœ¨è®­ç»ƒä¸­æ¥æ”¶çš„æ˜¯å®Œæ•´è¾“å…¥ï¼Œç¼ºä¹å¼ºçº¦æŸå»æ¿€æ´»å¤šæ ·åŒ–çš„è¡¨è¾¾ï¼›è€Œ Decoder é€šå¸¸å› ä¸ºè‡ªå›å½’è®­ç»ƒæœºåˆ¶ï¼Œè‡ªç„¶åœ°è¢«è¿«äº§ç”Ÿé«˜è¡¨è¾¾å¤šæ ·æ€§ï¼Œä»è€Œç¼“è§£äº†ä½ Rank çš„é£é™©ã€‚**

decoder-onlyæ”¯æŒä¸€ç›´å¤ç”¨KV-Cacheï¼Œå¯¹å¤šè½®å¯¹è¯æ›´å‹å¥½ï¼Œå› ä¸ºæ¯ä¸ªTokençš„è¡¨ç¤ºä¹‹å’Œå®ƒä¹‹å‰çš„è¾“å…¥æœ‰å…³ï¼Œè€Œencoder-decoderå’ŒPrefixLMå°±éš¾ä»¥åšåˆ°ã€‚

### Adamç®—æ³•

TODO

### æ¿€æ´»å‡½æ•°

ä¸åŒçš„æ¿€æ´»å‡½æ•°ï¼Œç‰¹ç‚¹å’Œä½œç”¨ä¸åŒï¼š

- `Sigmoid`å’Œ`tanh`çš„ç‰¹ç‚¹æ˜¯å°†è¾“å‡ºé™åˆ¶åœ¨`(0,1)`å’Œ`(-1,1)`ä¹‹é—´ï¼Œè¯´æ˜`Sigmoid`å’Œ`tanh`é€‚åˆåšæ¦‚ç‡å€¼çš„å¤„ç†ï¼Œä¾‹å¦‚LSTMä¸­çš„å„ç§é—¨ï¼›è€Œ`ReLU`å°±ä¸è¡Œï¼Œå› ä¸º`ReLU`æ— æœ€å¤§å€¼é™åˆ¶ï¼Œå¯èƒ½ä¼šå‡ºç°å¾ˆå¤§å€¼ã€‚
- `ReLU`é€‚åˆç”¨äºæ·±å±‚ç½‘ç»œçš„è®­ç»ƒï¼Œè€Œ`Sigmoid`å’Œ`tanh`åˆ™ä¸è¡Œï¼Œå› ä¸ºå®ƒä»¬ä¼šå‡ºç°æ¢¯åº¦æ¶ˆå¤±ã€‚

#### **æ®‹å·®è¿æ¥å’Œ normalize** 

è§£å†³æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸

#### ReLU

- å½“ `z>0` æ—¶ï¼ŒReLU æ¿€æ´»å‡½æ•°çš„å¯¼æ•°æ’ä¸ºå¸¸æ•°1ï¼Œè¿™å°±é¿å…äº† sigmoid å’Œ tanh ä¼šåœ¨ç¥ç»ç½‘ç»œå±‚æ•°æ¯”è¾ƒæ·±çš„æ—¶å€™å‡ºç°çš„æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜
- è®¡ç®—å¤æ‚åº¦ä½ï¼Œä¸å†å«æœ‰å¹‚è¿ç®—ï¼Œåªéœ€è¦ä¸€ä¸ªé˜ˆå€¼å°±èƒ½å¤Ÿå¾—åˆ°å…¶å¯¼æ•°ï¼›
- ç»è¿‡å®é™…å®éªŒå‘ç°ï¼Œ**ä½¿ç”¨ ReLU ä½œä¸ºæ¿€æ´»å‡½æ•°ï¼Œæ¨¡å‹æ”¶æ•›çš„é€Ÿåº¦æ¯” sigmoid å’Œ tanh å¿«**ï¼›
- å½“z<0æ—¶ï¼ŒReLU æ¿€æ´»å‡½æ•°çš„å¯¼æ•°æ’ä¸ºå¸¸æ•°0ï¼Œè¿™æ—¢å¸¦æ¥äº†ä¸€äº›æœ‰åˆ©çš„æ–¹é¢ï¼Œä¹Ÿå¯¼è‡´äº†ä¸€äº›åçš„æ–¹é¢ï¼Œåˆ†åˆ«è¿›è¡Œæè¿°ã€‚
  - æœ‰åˆ©çš„æ–¹é¢ï¼šåœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œç›®æ ‡æ˜¯ä»å¤§é‡æ•°æ®ä¸­å­¦ä¹ åˆ°å…³é”®ç‰¹å¾ï¼Œä¹Ÿå°±æ˜¯æŠŠå¯†é›†çŸ©é˜µè½¬åŒ–ä¸ºç¨€ç–çŸ©é˜µï¼Œä¿ç•™æ•°æ®çš„å…³é”®ä¿¡æ¯ï¼Œå»é™¤å™ªéŸ³ï¼Œè¿™æ ·çš„æ¨¡å‹å°±æœ‰äº†é²æ£’æ€§ã€‚ReLU æ¿€æ´»å‡½æ•°ä¸­å°† `z<0`çš„éƒ¨åˆ†ç½®ä¸º0ï¼Œå°±æ˜¯äº§ç”Ÿç¨€ç–çŸ©é˜µçš„è¿‡ç¨‹ã€‚
  - åçš„æ–¹é¢ï¼šå°† `z<0`çš„éƒ¨åˆ†æ¢¯åº¦ç›´æ¥ç½®ä¸º0ä¼šå¯¼è‡´ Dead ReLU Problem(ç¥ç»å…ƒåæ­»ç°è±¡)ã€‚**å¯èƒ½ä¼šå¯¼è‡´éƒ¨åˆ†ç¥ç»å…ƒä¸å†å¯¹è¾“å…¥æ•°æ®åšå“åº”ï¼Œæ— è®ºè¾“å…¥ä»€ä¹ˆæ•°æ®ï¼Œè¯¥éƒ¨åˆ†ç¥ç»å…ƒçš„å‚æ•°éƒ½ä¸ä¼šè¢«æ›´æ–°**ã€‚ï¼ˆè¿™ä¸ªé—®é¢˜æ˜¯ä¸€ä¸ªéå¸¸ä¸¥é‡çš„é—®é¢˜ï¼Œåç»­ä¸å°‘å·¥ä½œéƒ½æ˜¯åœ¨è§£å†³è¿™ä¸ªé—®é¢˜ï¼‰
- ReLU æœ‰å¯èƒ½ä¼šå¯¼è‡´æ¢¯åº¦çˆ†ç‚¸é—®é¢˜ï¼Œè§£å†³æ–¹æ³•æ˜¯æ¢¯åº¦æˆªæ–­ï¼›

### Prefix LM / Causal LM

å‰ç¼€è¯­è¨€æ¨¡å‹;å› æœè¯­è¨€æ¨¡å‹.

![img](https://wdndev.github.io/llm_interview_note/01.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/1.llm%E6%A6%82%E5%BF%B5/image/image_ZPQiHay1ZD.png)

### å¤§æ¨¡å‹LLMçš„ è®­ç»ƒç›®æ ‡

é¢„æµ‹ä¸‹ä¸€ä¸ªç›®æ ‡

Input: "The capital of France is"

æ¨¡å‹ä¼šæœ€å¤§åŒ–$P(\text{â€œParisâ€} \mid \text{â€œThe capital of France isâ€})$

| **é˜¶æ®µ**              | **ç›®æ ‡å‡½æ•°ä¸¾ä¾‹**                  | **ç›®çš„**                           |
| --------------------- | --------------------------------- | ---------------------------------- |
| 1ï¸âƒ£ é¢„è®­ç»ƒ              | è‡ªå›å½’/æ©ç è¯­è¨€å»ºæ¨¡æŸå¤±           | å­¦ä¹ é€šç”¨è¯­è¨€èƒ½åŠ›ï¼ˆè¯­æ³•+è¯­ä¹‰ï¼‰      |
| 2ï¸âƒ£ æŒ‡ä»¤å¾®è°ƒ            | æ•™æ¨¡å‹å¦‚ä½•å®Œæˆä»»åŠ¡ï¼ˆâ€œä½ æ˜¯åŠ©æ‰‹â€¦â€ï¼‰ | å­¦ä¹ ä»»åŠ¡æ ¼å¼ä¸æŒ‡ä»¤æ‰§è¡Œ             |
| 3ï¸âƒ£ å¼ºåŒ–å­¦ä¹ ï¼ˆå¦‚ RLHFï¼‰ | ä¼˜åŒ–è¾“å‡ºå¯¹äººç±»æœ‰ç”¨æ€§ï¼ˆåå¥½ï¼‰      | æå‡äººç±»æ»¡æ„åº¦ã€å¯¹è¯é€»è¾‘ã€ä»·å€¼å¯¹é½ |

### å¤§æ¨¡å‹å¤„ç†é•¿æ–‡æœ¬

å…ˆç”¨å¤§æ¨¡å‹æ¦‚æ‹¬æ–‡æœ¬ï¼Œå†ç»Ÿä¸€å¤„ç†è¡Œä¸è¡Œï¼Ÿï¼Ÿï¼Ÿ

è¦è®©å¤§æ¨¡å‹å¤„ç†æ›´é•¿çš„æ–‡æœ¬ï¼Œå¯ä»¥è€ƒè™‘ä»¥ä¸‹å‡ ä¸ªæ–¹æ³•ï¼š

1. **åˆ†å—å¤„ç†**ï¼šå°†é•¿æ–‡æœ¬åˆ†å‰²æˆè¾ƒçŸ­çš„ç‰‡æ®µï¼Œç„¶åé€ä¸ªç‰‡æ®µè¾“å…¥æ¨¡å‹è¿›è¡Œå¤„ç†ã€‚è¿™æ ·å¯ä»¥é¿å…é•¿æ–‡æœ¬å¯¹æ¨¡å‹å†…å­˜å’Œè®¡ç®—èµ„æºçš„å‹åŠ›ã€‚åœ¨å¤„ç†åˆ†å—æ–‡æœ¬æ—¶ï¼Œå¯ä»¥ä½¿ç”¨é‡å çš„æ–¹å¼ï¼Œå³å°†ç›¸é‚»ç‰‡æ®µçš„ä¸€éƒ¨åˆ†é‡å ï¼Œä»¥ä¿æŒä¸Šä¸‹æ–‡çš„è¿è´¯æ€§ã€‚
2. **å±‚æ¬¡å»ºæ¨¡**ï¼šé€šè¿‡å¼•å…¥å±‚æ¬¡ç»“æ„ï¼Œå°†é•¿æ–‡æœ¬åˆ’åˆ†ä¸ºæ›´å°çš„å•å…ƒã€‚ä¾‹å¦‚ï¼Œå¯ä»¥å°†æ–‡æœ¬åˆ†ä¸ºæ®µè½ã€å¥å­æˆ–å­å¥ç­‰å±‚æ¬¡ï¼Œç„¶åé€å±‚è¾“å…¥æ¨¡å‹è¿›è¡Œå¤„ç†ã€‚è¿™æ ·å¯ä»¥å‡å°‘æ¯ä¸ªå•å…ƒçš„é•¿åº¦ï¼Œæé«˜æ¨¡å‹å¤„ç†é•¿æ–‡æœ¬çš„èƒ½åŠ›ã€‚
3. **éƒ¨åˆ†ç”Ÿæˆ**ï¼šå¦‚æœåªéœ€è¦æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„ä¸€éƒ¨åˆ†ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ–‡æœ¬ï¼Œå¯ä»¥åªè¾“å…¥éƒ¨åˆ†æ–‡æœ¬ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œç„¶åè®©æ¨¡å‹ç”Ÿæˆæ‰€éœ€çš„éƒ¨åˆ†ã€‚ä¾‹å¦‚ï¼Œè¾“å…¥å‰ä¸€éƒ¨åˆ†æ–‡æœ¬ï¼Œè®©æ¨¡å‹ç”Ÿæˆåç»­çš„å†…å®¹ã€‚
4. **æ³¨æ„åŠ›æœºåˆ¶**ï¼šæ³¨æ„åŠ›æœºåˆ¶å¯ä»¥å¸®åŠ©æ¨¡å‹å…³æ³¨è¾“å…¥ä¸­çš„é‡è¦éƒ¨åˆ†ï¼Œå¯ä»¥ç”¨äºå¤„ç†é•¿æ–‡æœ¬æ—¶çš„ä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚é€šè¿‡å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°æ•æ‰é•¿æ–‡æœ¬ä¸­çš„å…³é”®ä¿¡æ¯ã€‚
5. **æ¨¡å‹ç»“æ„ä¼˜åŒ–**ï¼šé€šè¿‡ä¼˜åŒ–æ¨¡å‹ç»“æ„å’Œå‚æ•°è®¾ç½®ï¼Œå¯ä»¥æé«˜æ¨¡å‹å¤„ç†é•¿æ–‡æœ¬çš„èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥å¢åŠ æ¨¡å‹çš„å±‚æ•°æˆ–å‚æ•°é‡ï¼Œä»¥å¢åŠ æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚è¿˜å¯ä»¥ä½¿ç”¨æ›´é«˜æ•ˆçš„æ¨¡å‹æ¶æ„ï¼Œå¦‚Transformerç­‰ï¼Œä»¥æé«˜é•¿æ–‡æœ¬çš„å¤„ç†æ•ˆç‡ã€‚

### å¤§æ¨¡å‹æ¶Œç°èƒ½åŠ›

![img](https://picx.zhimg.com/v2-95fa4f87c3b4bcd4a9507ee7d8859231_1440w.jpg)

åˆ©ç”¨In Context Learningï¼Œå·²ç»å‘ç°åœ¨å„ç§ç±»å‹çš„ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œå¤§è¯­è¨€æ¨¡å‹éƒ½å‡ºç°äº†æ¶Œç°ç°è±¡ï¼Œä½“ç°åœ¨åœ¨æ¨¡å‹è§„æ¨¡ä¸å¤Ÿå¤§çš„æ—¶å€™ï¼Œå„ç§ä»»åŠ¡éƒ½å¤„ç†ä¸å¥½ï¼Œä½†æ˜¯å½“è·¨è¿‡æŸä¸ªæ¨¡å‹å¤§å°ä¸´ç•Œå€¼çš„æ—¶å€™ï¼Œå¤§æ¨¡å‹å°±çªç„¶èƒ½æ¯”è¾ƒå¥½åœ°å¤„ç†è¿™äº›ä»»åŠ¡ã€‚

ç¬¬äºŒç±»å…·å¤‡æ¶Œç°ç°è±¡çš„æŠ€æœ¯æ˜¯æ€ç»´é“¾( CoT)ã€‚CoTæœ¬è´¨ä¸Šæ˜¯ä¸€ç§ç‰¹æ®Šçš„few shot promptï¼Œå°±æ˜¯è¯´å¯¹äºæŸä¸ªå¤æ‚çš„æ¯”å¦‚æ¨ç†é—®é¢˜ï¼Œç”¨æˆ·æŠŠä¸€æ­¥ä¸€æ­¥çš„æ¨å¯¼è¿‡ç¨‹å†™å‡ºæ¥ï¼Œå¹¶æä¾›ç»™å¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ä¸‹å›¾è“è‰²æ–‡å­—å†…å®¹æ‰€ç¤ºï¼‰ï¼Œè¿™æ ·å¤§è¯­è¨€æ¨¡å‹å°±èƒ½åšä¸€äº›ç›¸å¯¹å¤æ‚çš„æ¨ç†ä»»åŠ¡ã€‚

**ä»»åŠ¡çš„è¯„ä»·æŒ‡æ ‡ä¸å¤Ÿå¹³æ»‘**ï¼šscaling law -> æ¶Œç°**Emergent Abilities**

æ¯ä¸ªå­ä»»åŠ¡å¹³æ»‘å¢é•¿ï¼Œæ€»ä½“èƒ½åŠ›æ¶Œç°

## Transformer

å‚æ•°ï¼šbatch size `B`, åºåˆ—é•¿åº¦tokenæ•°`T`ï¼Œæ¨¡å‹ç»´åº¦Dim`D`å¦‚512ï¼Œå¤´æ•°`H`æ¯”å¦‚8

```python
X: [B, T, D]
Q,K,V = Linear(X) -> [B, T, D]
reshapeå¤šå¤´
Q,K,V -> [B, H ,T ,D/H ]
attention scores = Q * K.transpose(-2, -1) -> [B, H, T, T]
attention weights = softmax(scores, dim=-1) -> [B, H, T, T]å¯¹æ¯ä¸ªQueryå¯¹åº”çš„Kç”Ÿæˆæ¦‚ç‡åˆ†å¸ƒ
```



```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SelfAttention(nn.Module):
    def __init__(self, embed_dim):
        super(SelfAttention, self).__init__()
        self.embed_dim = embed_dim
        # å®šä¹‰å¯å­¦ä¹ çš„çº¿æ€§å˜æ¢ï¼šQ, K, V
        self.query = nn.Linear(embed_dim, embed_dim)
        self.key = nn.Linear(embed_dim, embed_dim)
        self.value = nn.Linear(embed_dim, embed_dim)
        
        self.scale = embed_dim ** 0.5  # ç¼©æ”¾å› å­

    def forward(self, x):
        """
        x: shape [batch_size, seq_len, embed_dim]
        """
        Q = self.query(x)  # [B, L, D]
        K = self.key(x)    # [B, L, D]
        V = self.value(x)  # [B, L, D]
        # è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†ï¼ˆæ³¨æ„ï¼šKè¦è½¬ç½®ï¼‰
        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale 
        # [B, L, L]
        attn_weights = F.softmax(scores, dim=-1) # [B, L, L]
        # ç”¨æ³¨æ„åŠ›æƒé‡åŠ æƒV
        out = torch.matmul(attn_weights, V)  # [B, L, D]

        return out, attn_weights

class MultiHeadSelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super().__init__()
        assert embed_dim % num_heads == 0, "embed_dimå¿…é¡»èƒ½è¢«num_headsæ•´é™¤"
        
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        # æ˜ å°„åˆ°å¤šä¸ª Q, K, V
        self.qkv = nn.Linear(embed_dim, embed_dim * 3)  # ä¸€æ¬¡æ€§åš QKV æ˜ å°„
        self.out_proj = nn.Linear(embed_dim, embed_dim)

    def forward(self, x):
        B, L, D = x.shape
        qkv = self.qkv(x)  # [B, L, 3D]
        # reshape æˆ [B, L, 3, num_heads, head_dim]
        qkv = qkv.reshape(B, L, 3, self.num_heads, self.head_dim)
        # é‡æ’
        qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, B, H, L, head_dim]
        Q, K, V = qkv[0], qkv[1], qkv[2]  # æ¯ä¸ªæ˜¯ [B, H, L, head_dim]

        # Attention: QK^T / sqrt(d_k)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)  # [B, H, L, L]
        attn = F.softmax(scores, dim=-1)  # [B, H, L, L]

        out = torch.matmul(attn, V)  # [B, H, L, head_dim]
        out = out.transpose(1, 2).reshape(B, L, D)  # åˆå¹¶å¤šå¤´ï¼š[B, L, D]
        return self.out_proj(out)  # æœ€ç»ˆè¾“å‡ºå½¢çŠ¶ä»æ˜¯ [B, L, D]
```

#### self-attention åœ¨è®¡ç®—çš„è¿‡ç¨‹ä¸­ï¼Œå¦‚ä½•å¯¹paddingä½åšmaskï¼Ÿ

åœ¨ Attention æœºåˆ¶ä¸­ï¼ŒåŒæ ·éœ€è¦å¿½ç•¥ padding éƒ¨åˆ†çš„å½±å“ï¼Œè¿™é‡Œä»¥transformer encoderä¸­çš„self-attentionä¸ºä¾‹ï¼šself-attentionä¸­ï¼ŒQå’ŒKåœ¨ç‚¹ç§¯ä¹‹åï¼Œéœ€è¦å…ˆç»è¿‡maskå†è¿›è¡Œsoftmaxï¼Œå› æ­¤ï¼Œ**å¯¹äºè¦å±è”½çš„éƒ¨åˆ†ï¼Œmaskä¹‹åçš„è¾“å‡ºéœ€è¦ä¸ºè´Ÿæ— ç©·**ï¼Œè¿™æ ·softmaxä¹‹åè¾“å‡ºæ‰ä¸º0ã€‚

#### ä¸ºä»€ä¹ˆè¦multi-head

æ¯ä¸ªå¤´å¯ä»¥å­¦ä¹ åºåˆ—ä¸­ä¸åŒçš„å…³ç³»æ¨¡å¼ï¼Œä»å¤šä¸ªè§†è§’ç†è§£è¾“å…¥çš„æ•°æ®ã€‚æŠŠæ¯ä¸ªtokenå‘é‡é™ç»´åˆ°å¤šä¸ªå­ç©ºé—´ã€‚

### Flash Attention

![img](https://wdndev.github.io/llm_interview_note/02.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84/1.attention/image/image_JdHeyN9KuN.png)

FlashAttentionçš„ä¸»è¦åŠ¨æœºå°±æ˜¯**å¸Œæœ›æŠŠSRAMåˆ©ç”¨èµ·æ¥**ï¼Œä½†æ˜¯éš¾ç‚¹å°±åœ¨äºSRAMå¤ªå°äº†ï¼Œä¸€ä¸ªæ™®é€šçš„çŸ©é˜µä¹˜æ³•éƒ½æ”¾ä¸ä¸‹å»ã€‚FlashAttentionçš„è§£å†³æ€è·¯å°±æ˜¯å°†è®¡ç®—æ¨¡å—è¿›è¡Œåˆ†è§£ï¼Œæ‹†æˆä¸€ä¸ªä¸ªå°çš„è®¡ç®—ä»»åŠ¡ã€‚

#### Softmax Tiling

##### **ï¼ˆ1ï¼‰æ•°å€¼ç¨³å®š**

SoftmaxåŒ…å«æŒ‡æ•°å‡½æ•°ï¼Œæ‰€ä»¥ä¸ºäº†é¿å…æ•°å€¼æº¢å‡ºé—®é¢˜ï¼Œå¯ä»¥å°†æ¯ä¸ªå…ƒç´ éƒ½å‡å»æœ€å¤§å€¼ï¼Œå†ç»è¿‡e^{}ï¼Œæœ€åè®¡ç®—ç»“æœå’ŒåŸæ¥çš„Softmaxæ˜¯ä¸€è‡´çš„ã€‚

##### (2) åˆ†å—è®¡ç®—Softmax

Softmaxæ˜¯æŒ‰æ¯ä¸€ä¸ªQueryå»ç®—çš„ã€‚**å°†è¾“å…¥åˆ†å‰²æˆå—ï¼Œå¹¶åœ¨è¾“å…¥å—ä¸Šè¿›è¡Œå¤šæ¬¡ä¼ é€’ï¼Œä»è€Œä»¥å¢é‡æ–¹å¼æ‰§è¡Œsoftmaxç¼©å‡**ã€‚

- ä¸æ˜¾ç¤ºæ„é€ QT^KçŸ©é˜µ
  $$
  \mathrm{softmax}(S)V=\frac{e^S V}{\sum e^S}
  $$

  ```python
  # init 
  max_score, sum_exp, acc = -inf, 0, 0
  # æ¯ä¸€ä¸ªkey tile:
  s = q*k
  new_max  = max(max_score, max(s))
  sum_exp = e^{max_score - new_max} * sum_exp + \sum{e^{s-new_max}}
  acc = e^{max_score - new_max} * acc + \sum{e^{s-new_max}} * v
  out = acc / sum_exp
  ```

- 

![img](https://wdndev.github.io/llm_interview_note/02.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84/1.attention/image/image_bck1Jw3P5A.png)