---
layout:     post
title:     NLP Basic
subtitle:   LLM 
date:       2025-99-99
author:     SHK
header-img: img/post-bg-debug.png
catalog: true
tags: 
    - NLP
---

# NLP Basic

## 语言模型基础

### 1. Defination

A language model (LM) is classically defined as a probability distribution over sequences of tokens. Given a vocabulary of tokens $V$, a language model $p$assigns a probability (a number between 0 and 1) to each token sequence $x_1,\ldots,x_L \in V$, denoted as $p(x_1,…,x_L)$. 

### 2. Autoregressive LM

using the chain rule of probability: $p(x_{1:L})=\prod_{i=1}^{L}p(x_i|x_{1:i-1})$

### 3. Claude Elwood Shannon's theory

#### Information Entropy

Information entropy is a measure of the uncertainty of a **random variable**. The unit of information entropy is bit.
$$
H(X)=-\sum_{i=1}^{n}p_i \log_2{p_i}
$$

#### Self-information

$$
I(x)=-\log_2 p(x)
$$

#### Shannon's First Theorem**无失真信源编码定理**

在无噪声的情况下，信源输出的信息可以通过编码压缩到其信息熵的极限，并且可以无失真地恢复。

In the absence of noise, the information output by the information source can be compressed by coding to the limit of it's information entropy, and be restored without distortion. That is, the average code length L satisfies $H(x)\leq L < H(x)+1$

#### 香农第二定理（有噪信道编码定理，Shannon's Second Theorem）：

对于一个给定的有噪信道，其信道容量*C*是可以无差错传输信息的最大速率。如果信息传输速率*R*<*C*，则存在一种编码方式，使得在信道上传输信息的错误概率可以任意小；如果，则无论采用何种编码方式，错误概率都将大于零。

For a given noisy channel, its channel capacity *C* is the maximum rate at which information can be transmitted without error. If the information transmission rate *R*<*C*, there exists a coding method such that the error probability of information transmission over the channel can be arbitrarily small; if *R*>*C*, no matter what coding method is used, the error probability will be greater than zero.

#### Cross Entropy

$$
H(p,q)=-\sum_xp(x)\log q(x)
$$

**需要多少比特（nats）来编码样本x∼p，使用由模型q给出的压缩方案**

衡量了模型预测的概率分布与真实标签的概率分布之间的差异。

KL 散度也称为相对熵，用于衡量两个概率分布 $p$ 和 $q$ 之间的差异程度。
$$
D_{KL}(p||q)=H(p,q)-H(p)
$$
KL散度越大，说明两个分布差距越大。

### N-gram

**如果n太小，那么模型将无法捕获长距离的依赖关系**，下一个词将无法依赖于𝖲𝗍𝖺𝗇𝖿𝗈𝗋𝖽。然而，**如果n太大，统计上将无法得到概率的好估计**

### 词向量/分词

### 特征抽取

Transformer

### Bert

BERT（Bidirectional Encoder Representations from Transformers）是谷歌提出，作为一个Word2Vec的替代者，其在NLP领域的11个方向大幅刷新了精度，可以说是近年来自残差网络最优突破性的一项技术了（Encoder Only）。论文的主要特点以下几点：

1. 使用了双向Transformer作为算法的主要框架，之前的模型是从左向右输入一个文本序列，或者将 left-to-right 和 right-to-left 的训练结合起来，实验的结果表明，双向训练的语言模型对语境的理解会比单向的语言模型更深刻；
2. 使用了Mask Language Model(MLM)和 Next Sentence Prediction(NSP) 的多任务训练目标；
3. 使用更强大的机器训练更大规模的数据，使BERT的结果达到了全新的高度，并且Google开源了BERT模型，用户可以直接使用BERT作为Word2Vec的转换矩阵并高效的将其应用到自己的任务中。

两个任务：Masked LM；Next Sentence Prediction

### LLM为什么DecoderOnly

**Encoder 在抽取序列中某一个词的特征时能够看到整个序列中所有的信息，即上文和下文同时看到**；而 **Decoder 中因为有 mask 机制的存在，使得它在编码某一个词的特征时只能看到自身和它之前的文本信息**。

首先概述几种主要的架构:

- 以BERT为代表的**encoder-only**
- 以T5和BART为代表的**encoder-decoder**
- 以GPT为代表的**decoder-only**，

**Encoder 更容易出现低 Rank 问题，是因为它在训练中接收的是完整输入，缺乏强约束去激活多样化的表达；而 Decoder 通常因为自回归训练机制，自然地被迫产生高表达多样性，从而缓解了低 Rank 的风险。**

decoder-only支持一直复用KV-Cache，对多轮对话更友好，因为每个Token的表示之和它之前的输入有关，而encoder-decoder和PrefixLM就难以做到。

### Adam算法

TODO

### 激活函数

不同的激活函数，特点和作用不同：

- `Sigmoid`和`tanh`的特点是将输出限制在`(0,1)`和`(-1,1)`之间，说明`Sigmoid`和`tanh`适合做概率值的处理，例如LSTM中的各种门；而`ReLU`就不行，因为`ReLU`无最大值限制，可能会出现很大值。
- `ReLU`适合用于深层网络的训练，而`Sigmoid`和`tanh`则不行，因为它们会出现梯度消失。

#### **残差连接和 normalize** 

解决梯度消失/爆炸

#### ReLU

- 当 `z>0` 时，ReLU 激活函数的导数恒为常数1，这就避免了 sigmoid 和 tanh 会在神经网络层数比较深的时候出现的梯度消失的问题
- 计算复杂度低，不再含有幂运算，只需要一个阈值就能够得到其导数；
- 经过实际实验发现，**使用 ReLU 作为激活函数，模型收敛的速度比 sigmoid 和 tanh 快**；
- 当z<0时，ReLU 激活函数的导数恒为常数0，这既带来了一些有利的方面，也导致了一些坏的方面，分别进行描述。
  - 有利的方面：在深度学习中，目标是从大量数据中学习到关键特征，也就是把密集矩阵转化为稀疏矩阵，保留数据的关键信息，去除噪音，这样的模型就有了鲁棒性。ReLU 激活函数中将 `z<0`的部分置为0，就是产生稀疏矩阵的过程。
  - 坏的方面：将 `z<0`的部分梯度直接置为0会导致 Dead ReLU Problem(神经元坏死现象)。**可能会导致部分神经元不再对输入数据做响应，无论输入什么数据，该部分神经元的参数都不会被更新**。（这个问题是一个非常严重的问题，后续不少工作都是在解决这个问题）
- ReLU 有可能会导致梯度爆炸问题，解决方法是梯度截断；

### Prefix LM / Causal LM

前缀语言模型;因果语言模型.

![img](https://wdndev.github.io/llm_interview_note/01.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/1.llm%E6%A6%82%E5%BF%B5/image/image_ZPQiHay1ZD.png)

### 大模型LLM的 训练目标

预测下一个目标

Input: "The capital of France is"

模型会最大化$P(\text{“Paris”} \mid \text{“The capital of France is”})$

| **阶段**              | **目标函数举例**                  | **目的**                           |
| --------------------- | --------------------------------- | ---------------------------------- |
| 1️⃣ 预训练              | 自回归/掩码语言建模损失           | 学习通用语言能力（语法+语义）      |
| 2️⃣ 指令微调            | 教模型如何完成任务（“你是助手…”） | 学习任务格式与指令执行             |
| 3️⃣ 强化学习（如 RLHF） | 优化输出对人类有用性（偏好）      | 提升人类满意度、对话逻辑、价值对齐 |

### 大模型处理长文本

先用大模型概括文本，再统一处理行不行？？？

要让大模型处理更长的文本，可以考虑以下几个方法：

1. **分块处理**：将长文本分割成较短的片段，然后逐个片段输入模型进行处理。这样可以避免长文本对模型内存和计算资源的压力。在处理分块文本时，可以使用重叠的方式，即将相邻片段的一部分重叠，以保持上下文的连贯性。
2. **层次建模**：通过引入层次结构，将长文本划分为更小的单元。例如，可以将文本分为段落、句子或子句等层次，然后逐层输入模型进行处理。这样可以减少每个单元的长度，提高模型处理长文本的能力。
3. **部分生成**：如果只需要模型生成文本的一部分，而不是整个文本，可以只输入部分文本作为上下文，然后让模型生成所需的部分。例如，输入前一部分文本，让模型生成后续的内容。
4. **注意力机制**：注意力机制可以帮助模型关注输入中的重要部分，可以用于处理长文本时的上下文建模。通过引入注意力机制，模型可以更好地捕捉长文本中的关键信息。
5. **模型结构优化**：通过优化模型结构和参数设置，可以提高模型处理长文本的能力。例如，可以增加模型的层数或参数量，以增加模型的表达能力。还可以使用更高效的模型架构，如Transformer等，以提高长文本的处理效率。

### 大模型涌现能力

![img](https://picx.zhimg.com/v2-95fa4f87c3b4bcd4a9507ee7d8859231_1440w.jpg)

利用In Context Learning，已经发现在各种类型的下游任务中，大语言模型都出现了涌现现象，体现在在模型规模不够大的时候，各种任务都处理不好，但是当跨过某个模型大小临界值的时候，大模型就突然能比较好地处理这些任务。

第二类具备涌现现象的技术是思维链( CoT)。CoT本质上是一种特殊的few shot prompt，就是说对于某个复杂的比如推理问题，用户把一步一步的推导过程写出来，并提供给大语言模型（如下图蓝色文字内容所示），这样大语言模型就能做一些相对复杂的推理任务。

**任务的评价指标不够平滑**：scaling law -> 涌现**Emergent Abilities**

每个子任务平滑增长，总体能力涌现

## Transformer

参数：batch size `B`, 序列长度token数`T`，模型维度Dim`D`如512，头数`H`比如8

```python
X: [B, T, D]
Q,K,V = Linear(X) -> [B, T, D]
reshape多头
Q,K,V -> [B, H ,T ,D/H ]
attention scores = Q * K.transpose(-2, -1) -> [B, H, T, T]
attention weights = softmax(scores, dim=-1) -> [B, H, T, T]对每个Query对应的K生成概率分布
```



```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SelfAttention(nn.Module):
    def __init__(self, embed_dim):
        super(SelfAttention, self).__init__()
        self.embed_dim = embed_dim
        # 定义可学习的线性变换：Q, K, V
        self.query = nn.Linear(embed_dim, embed_dim)
        self.key = nn.Linear(embed_dim, embed_dim)
        self.value = nn.Linear(embed_dim, embed_dim)
        
        self.scale = embed_dim ** 0.5  # 缩放因子

    def forward(self, x):
        """
        x: shape [batch_size, seq_len, embed_dim]
        """
        Q = self.query(x)  # [B, L, D]
        K = self.key(x)    # [B, L, D]
        V = self.value(x)  # [B, L, D]
        # 计算注意力得分（注意：K要转置）
        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale 
        # [B, L, L]
        attn_weights = F.softmax(scores, dim=-1) # [B, L, L]
        # 用注意力权重加权V
        out = torch.matmul(attn_weights, V)  # [B, L, D]

        return out, attn_weights

class MultiHeadSelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super().__init__()
        assert embed_dim % num_heads == 0, "embed_dim必须能被num_heads整除"
        
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        # 映射到多个 Q, K, V
        self.qkv = nn.Linear(embed_dim, embed_dim * 3)  # 一次性做 QKV 映射
        self.out_proj = nn.Linear(embed_dim, embed_dim)

    def forward(self, x):
        B, L, D = x.shape
        qkv = self.qkv(x)  # [B, L, 3D]
        # reshape 成 [B, L, 3, num_heads, head_dim]
        qkv = qkv.reshape(B, L, 3, self.num_heads, self.head_dim)
        # 重排
        qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, B, H, L, head_dim]
        Q, K, V = qkv[0], qkv[1], qkv[2]  # 每个是 [B, H, L, head_dim]

        # Attention: QK^T / sqrt(d_k)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)  # [B, H, L, L]
        attn = F.softmax(scores, dim=-1)  # [B, H, L, L]

        out = torch.matmul(attn, V)  # [B, H, L, head_dim]
        out = out.transpose(1, 2).reshape(B, L, D)  # 合并多头：[B, L, D]
        return self.out_proj(out)  # 最终输出形状仍是 [B, L, D]
```

#### self-attention 在计算的过程中，如何对padding位做mask？

在 Attention 机制中，同样需要忽略 padding 部分的影响，这里以transformer encoder中的self-attention为例：self-attention中，Q和K在点积之后，需要先经过mask再进行softmax，因此，**对于要屏蔽的部分，mask之后的输出需要为负无穷**，这样softmax之后输出才为0。

#### 为什么要multi-head

每个头可以学习序列中不同的关系模式，从多个视角理解输入的数据。把每个token向量降维到多个子空间。

### Flash Attention

![img](https://wdndev.github.io/llm_interview_note/02.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84/1.attention/image/image_JdHeyN9KuN.png)

FlashAttention的主要动机就是**希望把SRAM利用起来**，但是难点就在于SRAM太小了，一个普通的矩阵乘法都放不下去。FlashAttention的解决思路就是将计算模块进行分解，拆成一个个小的计算任务。

#### Softmax Tiling

##### **（1）数值稳定**

Softmax包含指数函数，所以为了避免数值溢出问题，可以将每个元素都减去最大值，再经过e^{}，最后计算结果和原来的Softmax是一致的。

##### (2) 分块计算Softmax

Softmax是按每一个Query去算的。**将输入分割成块，并在输入块上进行多次传递，从而以增量方式执行softmax缩减**。

- 不显示构造QT^K矩阵
  $$
  \mathrm{softmax}(S)V=\frac{e^S V}{\sum e^S}
  $$

  ```python
  # init 
  max_score, sum_exp, acc = -inf, 0, 0
  # 每一个key tile:
  s = q*k
  new_max  = max(max_score, max(s))
  sum_exp = e^{max_score - new_max} * sum_exp + \sum{e^{s-new_max}}
  acc = e^{max_score - new_max} * acc + \sum{e^{s-new_max}} * v
  out = acc / sum_exp
  ```

- 

![img](https://wdndev.github.io/llm_interview_note/02.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84/1.attention/image/image_bck1Jw3P5A.png)