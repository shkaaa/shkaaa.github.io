---
layout:     post
title:     NLP Basic
subtitle:   LLM 
date:       2025-05-05
author:     SHK
header-img: img/post-bg-debug.png
catalog: true
tags: 
    - NLP
---

# NLP Basic

## è¯­è¨€æ¨¡å‹åŸºç¡€

### 1. Defination

A language model (LM) is classically defined as a probability distribution over sequences of tokens. Given a vocabulary of tokens $V$, a language model $p$assigns a probability (a number between 0 and 1) to each token sequence $x_1,\ldots,x_L \in V$, denoted as $p(x_1,â€¦,x_L)$. 

### 2. Autoregressive LM

using the chain rule of probability: $p(x_{1:L})=\prod_{i=1}^{L}p(x_i|x_{1:i-1})$

### 3. Claude Elwood Shannon's theory

#### Information Entropy

Information entropy is a measure of the uncertainty of a **random variable**. The unit of information entropy is bit.
$$
H(X)=-\sum_{i=1}^{n}p_i \log_2{p_i}
$$

#### Self-information

$$
I(x)=-\log_2 p(x)
$$

#### Shannon's First Theorem**æ— å¤±çœŸä¿¡æºç¼–ç å®šç†**

åœ¨æ— å™ªå£°çš„æƒ…å†µä¸‹ï¼Œä¿¡æºè¾“å‡ºçš„ä¿¡æ¯å¯ä»¥é€šè¿‡ç¼–ç å‹ç¼©åˆ°å…¶ä¿¡æ¯ç†µçš„æé™ï¼Œå¹¶ä¸”å¯ä»¥æ— å¤±çœŸåœ°æ¢å¤ã€‚

In the absence of noise, the information output by the information source can be compressed by coding to the limit of it's information entropy, and be restored without distortion. That is, the average code length L satisfies $H(x)\leq L < H(x)+1$

#### é¦™å†œç¬¬äºŒå®šç†ï¼ˆæœ‰å™ªä¿¡é“ç¼–ç å®šç†ï¼ŒShannon's Second Theoremï¼‰ï¼š

å¯¹äºä¸€ä¸ªç»™å®šçš„æœ‰å™ªä¿¡é“ï¼Œå…¶ä¿¡é“å®¹é‡*C*æ˜¯å¯ä»¥æ— å·®é”™ä¼ è¾“ä¿¡æ¯çš„æœ€å¤§é€Ÿç‡ã€‚å¦‚æœä¿¡æ¯ä¼ è¾“é€Ÿç‡*R*<*C*ï¼Œåˆ™å­˜åœ¨ä¸€ç§ç¼–ç æ–¹å¼ï¼Œä½¿å¾—åœ¨ä¿¡é“ä¸Šä¼ è¾“ä¿¡æ¯çš„é”™è¯¯æ¦‚ç‡å¯ä»¥ä»»æ„å°ï¼›å¦‚æœï¼Œåˆ™æ— è®ºé‡‡ç”¨ä½•ç§ç¼–ç æ–¹å¼ï¼Œé”™è¯¯æ¦‚ç‡éƒ½å°†å¤§äºé›¶ã€‚

For a given noisy channel, its channel capacity *C* is the maximum rate at which information can be transmitted without error. If the information transmission rate *R*<*C*, there exists a coding method such that the error probability of information transmission over the channel can be arbitrarily small; if *R*>*C*, no matter what coding method is used, the error probability will be greater than zero.

#### Cross Entropy

$$
H(p,q)=-\sum_xp(x)\log q(x)
$$

**éœ€è¦å¤šå°‘æ¯”ç‰¹ï¼ˆnatsï¼‰æ¥ç¼–ç æ ·æœ¬xâˆ¼pï¼Œä½¿ç”¨ç”±æ¨¡å‹qç»™å‡ºçš„å‹ç¼©æ–¹æ¡ˆ**

è¡¡é‡äº†æ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒä¸çœŸå®æ ‡ç­¾çš„æ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚

KL æ•£åº¦ä¹Ÿç§°ä¸ºç›¸å¯¹ç†µï¼Œç”¨äºè¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒ $p$ å’Œ $q$ ä¹‹é—´çš„å·®å¼‚ç¨‹åº¦ã€‚
$$
D_{KL}(p||q)=H(p,q)-H(p)
$$
KLæ•£åº¦è¶Šå¤§ï¼Œè¯´æ˜ä¸¤ä¸ªåˆ†å¸ƒå·®è·è¶Šå¤§ã€‚

### N-gram

**å¦‚æœnå¤ªå°ï¼Œé‚£ä¹ˆæ¨¡å‹å°†æ— æ³•æ•è·é•¿è·ç¦»çš„ä¾èµ–å…³ç³»**ï¼Œä¸‹ä¸€ä¸ªè¯å°†æ— æ³•ä¾èµ–äºğ–²ğ—ğ–ºğ—‡ğ–¿ğ—ˆğ—‹ğ–½ã€‚ç„¶è€Œï¼Œ**å¦‚æœnå¤ªå¤§ï¼Œç»Ÿè®¡ä¸Šå°†æ— æ³•å¾—åˆ°æ¦‚ç‡çš„å¥½ä¼°è®¡**

### ç‰¹å¾æŠ½å–

Transformer

### Bert

BERTï¼ˆBidirectional Encoder Representations from Transformersï¼‰æ˜¯è°·æ­Œæå‡ºï¼Œä½œä¸ºä¸€ä¸ªWord2Vecçš„æ›¿ä»£è€…ï¼Œå…¶åœ¨NLPé¢†åŸŸçš„11ä¸ªæ–¹å‘å¤§å¹…åˆ·æ–°äº†ç²¾åº¦ï¼Œå¯ä»¥è¯´æ˜¯è¿‘å¹´æ¥è‡ªæ®‹å·®ç½‘ç»œæœ€ä¼˜çªç ´æ€§çš„ä¸€é¡¹æŠ€æœ¯äº†ï¼ˆEncoder Onlyï¼‰ã€‚è®ºæ–‡çš„ä¸»è¦ç‰¹ç‚¹ä»¥ä¸‹å‡ ç‚¹ï¼š

1. ä½¿ç”¨äº†åŒå‘Transformerä½œä¸ºç®—æ³•çš„ä¸»è¦æ¡†æ¶ï¼Œä¹‹å‰çš„æ¨¡å‹æ˜¯ä»å·¦å‘å³è¾“å…¥ä¸€ä¸ªæ–‡æœ¬åºåˆ—ï¼Œæˆ–è€…å°† left-to-right å’Œ right-to-left çš„è®­ç»ƒç»“åˆèµ·æ¥ï¼Œå®éªŒçš„ç»“æœè¡¨æ˜ï¼ŒåŒå‘è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å¯¹è¯­å¢ƒçš„ç†è§£ä¼šæ¯”å•å‘çš„è¯­è¨€æ¨¡å‹æ›´æ·±åˆ»ï¼›
2. ä½¿ç”¨äº†Mask Language Model(MLM)å’Œ Next Sentence Prediction(NSP) çš„å¤šä»»åŠ¡è®­ç»ƒç›®æ ‡ï¼›
3. ä½¿ç”¨æ›´å¼ºå¤§çš„æœºå™¨è®­ç»ƒæ›´å¤§è§„æ¨¡çš„æ•°æ®ï¼Œä½¿BERTçš„ç»“æœè¾¾åˆ°äº†å…¨æ–°çš„é«˜åº¦ï¼Œå¹¶ä¸”Googleå¼€æºäº†BERTæ¨¡å‹ï¼Œç”¨æˆ·å¯ä»¥ç›´æ¥ä½¿ç”¨BERTä½œä¸ºWord2Vecçš„è½¬æ¢çŸ©é˜µå¹¶é«˜æ•ˆçš„å°†å…¶åº”ç”¨åˆ°è‡ªå·±çš„ä»»åŠ¡ä¸­ã€‚

ä¸¤ä¸ªä»»åŠ¡ï¼šMasked LMï¼›Next Sentence Prediction

### LLMä¸ºä»€ä¹ˆDecoderOnly

**Encoder åœ¨æŠ½å–åºåˆ—ä¸­æŸä¸€ä¸ªè¯çš„ç‰¹å¾æ—¶èƒ½å¤Ÿçœ‹åˆ°æ•´ä¸ªåºåˆ—ä¸­æ‰€æœ‰çš„ä¿¡æ¯ï¼Œå³ä¸Šæ–‡å’Œä¸‹æ–‡åŒæ—¶çœ‹åˆ°**ï¼›è€Œ **Decoder ä¸­å› ä¸ºæœ‰ mask æœºåˆ¶çš„å­˜åœ¨ï¼Œä½¿å¾—å®ƒåœ¨ç¼–ç æŸä¸€ä¸ªè¯çš„ç‰¹å¾æ—¶åªèƒ½çœ‹åˆ°è‡ªèº«å’Œå®ƒä¹‹å‰çš„æ–‡æœ¬ä¿¡æ¯**ã€‚

é¦–å…ˆæ¦‚è¿°å‡ ç§ä¸»è¦çš„æ¶æ„:

- ä»¥BERTä¸ºä»£è¡¨çš„**encoder-only**
- ä»¥T5å’ŒBARTä¸ºä»£è¡¨çš„**encoder-decoder**
- ä»¥GPTä¸ºä»£è¡¨çš„**decoder-only**ï¼Œ

**Encoder æ›´å®¹æ˜“å‡ºç°ä½ Rank é—®é¢˜ï¼Œæ˜¯å› ä¸ºå®ƒåœ¨è®­ç»ƒä¸­æ¥æ”¶çš„æ˜¯å®Œæ•´è¾“å…¥ï¼Œç¼ºä¹å¼ºçº¦æŸå»æ¿€æ´»å¤šæ ·åŒ–çš„è¡¨è¾¾ï¼›è€Œ Decoder é€šå¸¸å› ä¸ºè‡ªå›å½’è®­ç»ƒæœºåˆ¶ï¼Œè‡ªç„¶åœ°è¢«è¿«äº§ç”Ÿé«˜è¡¨è¾¾å¤šæ ·æ€§ï¼Œä»è€Œç¼“è§£äº†ä½ Rank çš„é£é™©ã€‚**

decoder-onlyæ”¯æŒä¸€ç›´å¤ç”¨KV-Cacheï¼Œå¯¹å¤šè½®å¯¹è¯æ›´å‹å¥½ï¼Œå› ä¸ºæ¯ä¸ªTokençš„è¡¨ç¤ºä¹‹å’Œå®ƒä¹‹å‰çš„è¾“å…¥æœ‰å…³ï¼Œè€Œencoder-decoderå’ŒPrefixLMå°±éš¾ä»¥åšåˆ°ã€‚

### Adamç®—æ³•

TODO

### æ¿€æ´»å‡½æ•°

ä¸åŒçš„æ¿€æ´»å‡½æ•°ï¼Œç‰¹ç‚¹å’Œä½œç”¨ä¸åŒï¼š

- `Sigmoid`å’Œ`tanh`çš„ç‰¹ç‚¹æ˜¯å°†è¾“å‡ºé™åˆ¶åœ¨`(0,1)`å’Œ`(-1,1)`ä¹‹é—´ï¼Œè¯´æ˜`Sigmoid`å’Œ`tanh`é€‚åˆåšæ¦‚ç‡å€¼çš„å¤„ç†ï¼Œä¾‹å¦‚LSTMä¸­çš„å„ç§é—¨ï¼›è€Œ`ReLU`å°±ä¸è¡Œï¼Œå› ä¸º`ReLU`æ— æœ€å¤§å€¼é™åˆ¶ï¼Œå¯èƒ½ä¼šå‡ºç°å¾ˆå¤§å€¼ã€‚
- `ReLU`é€‚åˆç”¨äºæ·±å±‚ç½‘ç»œçš„è®­ç»ƒï¼Œè€Œ`Sigmoid`å’Œ`tanh`åˆ™ä¸è¡Œï¼Œå› ä¸ºå®ƒä»¬ä¼šå‡ºç°æ¢¯åº¦æ¶ˆå¤±ã€‚

#### **æ®‹å·®è¿æ¥å’Œ normalize** 

è§£å†³æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸

#### ReLU

- å½“ `z>0` æ—¶ï¼ŒReLU æ¿€æ´»å‡½æ•°çš„å¯¼æ•°æ’ä¸ºå¸¸æ•°1ï¼Œè¿™å°±é¿å…äº† sigmoid å’Œ tanh ä¼šåœ¨ç¥ç»ç½‘ç»œå±‚æ•°æ¯”è¾ƒæ·±çš„æ—¶å€™å‡ºç°çš„æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜
- è®¡ç®—å¤æ‚åº¦ä½ï¼Œä¸å†å«æœ‰å¹‚è¿ç®—ï¼Œåªéœ€è¦ä¸€ä¸ªé˜ˆå€¼å°±èƒ½å¤Ÿå¾—åˆ°å…¶å¯¼æ•°ï¼›
- ç»è¿‡å®é™…å®éªŒå‘ç°ï¼Œ**ä½¿ç”¨ ReLU ä½œä¸ºæ¿€æ´»å‡½æ•°ï¼Œæ¨¡å‹æ”¶æ•›çš„é€Ÿåº¦æ¯” sigmoid å’Œ tanh å¿«**ï¼›
- å½“z<0æ—¶ï¼ŒReLU æ¿€æ´»å‡½æ•°çš„å¯¼æ•°æ’ä¸ºå¸¸æ•°0ï¼Œè¿™æ—¢å¸¦æ¥äº†ä¸€äº›æœ‰åˆ©çš„æ–¹é¢ï¼Œä¹Ÿå¯¼è‡´äº†ä¸€äº›åçš„æ–¹é¢ï¼Œåˆ†åˆ«è¿›è¡Œæè¿°ã€‚
  - æœ‰åˆ©çš„æ–¹é¢ï¼šåœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œç›®æ ‡æ˜¯ä»å¤§é‡æ•°æ®ä¸­å­¦ä¹ åˆ°å…³é”®ç‰¹å¾ï¼Œä¹Ÿå°±æ˜¯æŠŠå¯†é›†çŸ©é˜µè½¬åŒ–ä¸ºç¨€ç–çŸ©é˜µï¼Œä¿ç•™æ•°æ®çš„å…³é”®ä¿¡æ¯ï¼Œå»é™¤å™ªéŸ³ï¼Œè¿™æ ·çš„æ¨¡å‹å°±æœ‰äº†é²æ£’æ€§ã€‚ReLU æ¿€æ´»å‡½æ•°ä¸­å°† `z<0`çš„éƒ¨åˆ†ç½®ä¸º0ï¼Œå°±æ˜¯äº§ç”Ÿç¨€ç–çŸ©é˜µçš„è¿‡ç¨‹ã€‚
  - åçš„æ–¹é¢ï¼šå°† `z<0`çš„éƒ¨åˆ†æ¢¯åº¦ç›´æ¥ç½®ä¸º0ä¼šå¯¼è‡´ Dead ReLU Problem(ç¥ç»å…ƒåæ­»ç°è±¡)ã€‚**å¯èƒ½ä¼šå¯¼è‡´éƒ¨åˆ†ç¥ç»å…ƒä¸å†å¯¹è¾“å…¥æ•°æ®åšå“åº”ï¼Œæ— è®ºè¾“å…¥ä»€ä¹ˆæ•°æ®ï¼Œè¯¥éƒ¨åˆ†ç¥ç»å…ƒçš„å‚æ•°éƒ½ä¸ä¼šè¢«æ›´æ–°**ã€‚ï¼ˆè¿™ä¸ªé—®é¢˜æ˜¯ä¸€ä¸ªéå¸¸ä¸¥é‡çš„é—®é¢˜ï¼Œåç»­ä¸å°‘å·¥ä½œéƒ½æ˜¯åœ¨è§£å†³è¿™ä¸ªé—®é¢˜ï¼‰
- ReLU æœ‰å¯èƒ½ä¼šå¯¼è‡´æ¢¯åº¦çˆ†ç‚¸é—®é¢˜ï¼Œè§£å†³æ–¹æ³•æ˜¯æ¢¯åº¦æˆªæ–­ï¼›

### Prefix LM / Causal LM

å‰ç¼€è¯­è¨€æ¨¡å‹;å› æœè¯­è¨€æ¨¡å‹.

![img](https://wdndev.github.io/llm_interview_note/01.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/1.llm%E6%A6%82%E5%BF%B5/image/image_ZPQiHay1ZD.png)

### å¤§æ¨¡å‹LLMçš„ è®­ç»ƒç›®æ ‡

é¢„æµ‹ä¸‹ä¸€ä¸ªç›®æ ‡

Input: "The capital of France is"

æ¨¡å‹ä¼šæœ€å¤§åŒ–$P(\text{â€œParisâ€} \mid \text{â€œThe capital of France isâ€})$

| **é˜¶æ®µ**              | **ç›®æ ‡å‡½æ•°ä¸¾ä¾‹**                  | **ç›®çš„**                           |
| --------------------- | --------------------------------- | ---------------------------------- |
| 1ï¸âƒ£ é¢„è®­ç»ƒ              | è‡ªå›å½’/æ©ç è¯­è¨€å»ºæ¨¡æŸå¤±           | å­¦ä¹ é€šç”¨è¯­è¨€èƒ½åŠ›ï¼ˆè¯­æ³•+è¯­ä¹‰ï¼‰      |
| 2ï¸âƒ£ æŒ‡ä»¤å¾®è°ƒ            | æ•™æ¨¡å‹å¦‚ä½•å®Œæˆä»»åŠ¡ï¼ˆâ€œä½ æ˜¯åŠ©æ‰‹â€¦â€ï¼‰ | å­¦ä¹ ä»»åŠ¡æ ¼å¼ä¸æŒ‡ä»¤æ‰§è¡Œ             |
| 3ï¸âƒ£ å¼ºåŒ–å­¦ä¹ ï¼ˆå¦‚ RLHFï¼‰ | ä¼˜åŒ–è¾“å‡ºå¯¹äººç±»æœ‰ç”¨æ€§ï¼ˆåå¥½ï¼‰      | æå‡äººç±»æ»¡æ„åº¦ã€å¯¹è¯é€»è¾‘ã€ä»·å€¼å¯¹é½ |

### å¤§æ¨¡å‹å¤„ç†é•¿æ–‡æœ¬

å…ˆç”¨å¤§æ¨¡å‹æ¦‚æ‹¬æ–‡æœ¬ï¼Œå†ç»Ÿä¸€å¤„ç†è¡Œä¸è¡Œï¼Ÿï¼Ÿï¼Ÿ

è¦è®©å¤§æ¨¡å‹å¤„ç†æ›´é•¿çš„æ–‡æœ¬ï¼Œå¯ä»¥è€ƒè™‘ä»¥ä¸‹å‡ ä¸ªæ–¹æ³•ï¼š

1. **åˆ†å—å¤„ç†**ï¼šå°†é•¿æ–‡æœ¬åˆ†å‰²æˆè¾ƒçŸ­çš„ç‰‡æ®µï¼Œç„¶åé€ä¸ªç‰‡æ®µè¾“å…¥æ¨¡å‹è¿›è¡Œå¤„ç†ã€‚è¿™æ ·å¯ä»¥é¿å…é•¿æ–‡æœ¬å¯¹æ¨¡å‹å†…å­˜å’Œè®¡ç®—èµ„æºçš„å‹åŠ›ã€‚åœ¨å¤„ç†åˆ†å—æ–‡æœ¬æ—¶ï¼Œå¯ä»¥ä½¿ç”¨é‡å çš„æ–¹å¼ï¼Œå³å°†ç›¸é‚»ç‰‡æ®µçš„ä¸€éƒ¨åˆ†é‡å ï¼Œä»¥ä¿æŒä¸Šä¸‹æ–‡çš„è¿è´¯æ€§ã€‚
2. **å±‚æ¬¡å»ºæ¨¡**ï¼šé€šè¿‡å¼•å…¥å±‚æ¬¡ç»“æ„ï¼Œå°†é•¿æ–‡æœ¬åˆ’åˆ†ä¸ºæ›´å°çš„å•å…ƒã€‚ä¾‹å¦‚ï¼Œå¯ä»¥å°†æ–‡æœ¬åˆ†ä¸ºæ®µè½ã€å¥å­æˆ–å­å¥ç­‰å±‚æ¬¡ï¼Œç„¶åé€å±‚è¾“å…¥æ¨¡å‹è¿›è¡Œå¤„ç†ã€‚è¿™æ ·å¯ä»¥å‡å°‘æ¯ä¸ªå•å…ƒçš„é•¿åº¦ï¼Œæé«˜æ¨¡å‹å¤„ç†é•¿æ–‡æœ¬çš„èƒ½åŠ›ã€‚
3. **éƒ¨åˆ†ç”Ÿæˆ**ï¼šå¦‚æœåªéœ€è¦æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„ä¸€éƒ¨åˆ†ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ–‡æœ¬ï¼Œå¯ä»¥åªè¾“å…¥éƒ¨åˆ†æ–‡æœ¬ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œç„¶åè®©æ¨¡å‹ç”Ÿæˆæ‰€éœ€çš„éƒ¨åˆ†ã€‚ä¾‹å¦‚ï¼Œè¾“å…¥å‰ä¸€éƒ¨åˆ†æ–‡æœ¬ï¼Œè®©æ¨¡å‹ç”Ÿæˆåç»­çš„å†…å®¹ã€‚
4. **æ³¨æ„åŠ›æœºåˆ¶**ï¼šæ³¨æ„åŠ›æœºåˆ¶å¯ä»¥å¸®åŠ©æ¨¡å‹å…³æ³¨è¾“å…¥ä¸­çš„é‡è¦éƒ¨åˆ†ï¼Œå¯ä»¥ç”¨äºå¤„ç†é•¿æ–‡æœ¬æ—¶çš„ä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚é€šè¿‡å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°æ•æ‰é•¿æ–‡æœ¬ä¸­çš„å…³é”®ä¿¡æ¯ã€‚
5. **æ¨¡å‹ç»“æ„ä¼˜åŒ–**ï¼šé€šè¿‡ä¼˜åŒ–æ¨¡å‹ç»“æ„å’Œå‚æ•°è®¾ç½®ï¼Œå¯ä»¥æé«˜æ¨¡å‹å¤„ç†é•¿æ–‡æœ¬çš„èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥å¢åŠ æ¨¡å‹çš„å±‚æ•°æˆ–å‚æ•°é‡ï¼Œä»¥å¢åŠ æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚è¿˜å¯ä»¥ä½¿ç”¨æ›´é«˜æ•ˆçš„æ¨¡å‹æ¶æ„ï¼Œå¦‚Transformerç­‰ï¼Œä»¥æé«˜é•¿æ–‡æœ¬çš„å¤„ç†æ•ˆç‡ã€‚

### å¤§æ¨¡å‹æ¶Œç°èƒ½åŠ›

![img](https://picx.zhimg.com/v2-95fa4f87c3b4bcd4a9507ee7d8859231_1440w.jpg)

åˆ©ç”¨In Context Learningï¼Œå·²ç»å‘ç°åœ¨å„ç§ç±»å‹çš„ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œå¤§è¯­è¨€æ¨¡å‹éƒ½å‡ºç°äº†æ¶Œç°ç°è±¡ï¼Œä½“ç°åœ¨åœ¨æ¨¡å‹è§„æ¨¡ä¸å¤Ÿå¤§çš„æ—¶å€™ï¼Œå„ç§ä»»åŠ¡éƒ½å¤„ç†ä¸å¥½ï¼Œä½†æ˜¯å½“è·¨è¿‡æŸä¸ªæ¨¡å‹å¤§å°ä¸´ç•Œå€¼çš„æ—¶å€™ï¼Œå¤§æ¨¡å‹å°±çªç„¶èƒ½æ¯”è¾ƒå¥½åœ°å¤„ç†è¿™äº›ä»»åŠ¡ã€‚

ç¬¬äºŒç±»å…·å¤‡æ¶Œç°ç°è±¡çš„æŠ€æœ¯æ˜¯æ€ç»´é“¾( CoT)ã€‚CoTæœ¬è´¨ä¸Šæ˜¯ä¸€ç§ç‰¹æ®Šçš„few shot promptï¼Œå°±æ˜¯è¯´å¯¹äºæŸä¸ªå¤æ‚çš„æ¯”å¦‚æ¨ç†é—®é¢˜ï¼Œç”¨æˆ·æŠŠä¸€æ­¥ä¸€æ­¥çš„æ¨å¯¼è¿‡ç¨‹å†™å‡ºæ¥ï¼Œå¹¶æä¾›ç»™å¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ä¸‹å›¾è“è‰²æ–‡å­—å†…å®¹æ‰€ç¤ºï¼‰ï¼Œè¿™æ ·å¤§è¯­è¨€æ¨¡å‹å°±èƒ½åšä¸€äº›ç›¸å¯¹å¤æ‚çš„æ¨ç†ä»»åŠ¡ã€‚

**ä»»åŠ¡çš„è¯„ä»·æŒ‡æ ‡ä¸å¤Ÿå¹³æ»‘**ï¼šscaling law -> æ¶Œç°**Emergent Abilities**

æ¯ä¸ªå­ä»»åŠ¡å¹³æ»‘å¢é•¿ï¼Œæ€»ä½“èƒ½åŠ›æ¶Œç°

## Transformer

å‚æ•°ï¼šbatch size `B`, åºåˆ—é•¿åº¦tokenæ•°`T`ï¼Œæ¨¡å‹ç»´åº¦Dim`D`å¦‚512ï¼Œå¤´æ•°`H`æ¯”å¦‚8

```python
X: [B, T, D]
Q,K,V = Linear(X) -> [B, T, D]
reshapeå¤šå¤´
Q,K,V -> [B, H ,T ,D/H ]
attention scores = Q * K.transpose(-2, -1) -> [B, H, T, T]
attention weights = softmax(scores, dim=-1) -> [B, H, T, T]å¯¹æ¯ä¸ªQueryå¯¹åº”çš„Kç”Ÿæˆæ¦‚ç‡åˆ†å¸ƒ
```



```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SelfAttention(nn.Module):
    def __init__(self, embed_dim):
        super(SelfAttention, self).__init__()
        self.embed_dim = embed_dim
        # å®šä¹‰å¯å­¦ä¹ çš„çº¿æ€§å˜æ¢ï¼šQ, K, V
        self.query = nn.Linear(embed_dim, embed_dim)
        self.key = nn.Linear(embed_dim, embed_dim)
        self.value = nn.Linear(embed_dim, embed_dim)
        
        self.scale = embed_dim ** 0.5  # ç¼©æ”¾å› å­

    def forward(self, x):
        """
        x: shape [batch_size, seq_len, embed_dim]
        """
        Q = self.query(x)  # [B, L, D]
        K = self.key(x)    # [B, L, D]
        V = self.value(x)  # [B, L, D]
        # è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†ï¼ˆæ³¨æ„ï¼šKè¦è½¬ç½®ï¼‰
        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale 
        # [B, L, L]
        attn_weights = F.softmax(scores, dim=-1) # [B, L, L]
        # ç”¨æ³¨æ„åŠ›æƒé‡åŠ æƒV
        out = torch.matmul(attn_weights, V)  # [B, L, D]

        return out, attn_weights

class MultiHeadSelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super().__init__()
        assert embed_dim % num_heads == 0, "embed_dimå¿…é¡»èƒ½è¢«num_headsæ•´é™¤"
        
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        # æ˜ å°„åˆ°å¤šä¸ª Q, K, V
        self.qkv = nn.Linear(embed_dim, embed_dim * 3)  # ä¸€æ¬¡æ€§åš QKV æ˜ å°„
        self.out_proj = nn.Linear(embed_dim, embed_dim)

    def forward(self, x):
        B, L, D = x.shape
        qkv = self.qkv(x)  # [B, L, 3D]
        # reshape æˆ [B, L, 3, num_heads, head_dim]
        qkv = qkv.reshape(B, L, 3, self.num_heads, self.head_dim)
        # é‡æ’
        qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, B, H, L, head_dim]
        Q, K, V = qkv[0], qkv[1], qkv[2]  # æ¯ä¸ªæ˜¯ [B, H, L, head_dim]

        # Attention: QK^T / sqrt(d_k)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)  # [B, H, L, L]
        attn = F.softmax(scores, dim=-1)  # [B, H, L, L]

        out = torch.matmul(attn, V)  # [B, H, L, head_dim]
        out = out.transpose(1, 2).reshape(B, L, D)  # åˆå¹¶å¤šå¤´ï¼š[B, L, D]
        return self.out_proj(out)  # æœ€ç»ˆè¾“å‡ºå½¢çŠ¶ä»æ˜¯ [B, L, D]
```

#### self-attention åœ¨è®¡ç®—çš„è¿‡ç¨‹ä¸­ï¼Œå¦‚ä½•å¯¹paddingä½åšmaskï¼Ÿ

åœ¨ Attention æœºåˆ¶ä¸­ï¼ŒåŒæ ·éœ€è¦å¿½ç•¥ padding éƒ¨åˆ†çš„å½±å“ï¼Œè¿™é‡Œä»¥transformer encoderä¸­çš„self-attentionä¸ºä¾‹ï¼šself-attentionä¸­ï¼ŒQå’ŒKåœ¨ç‚¹ç§¯ä¹‹åï¼Œéœ€è¦å…ˆç»è¿‡maskå†è¿›è¡Œsoftmaxï¼Œå› æ­¤ï¼Œ**å¯¹äºè¦å±è”½çš„éƒ¨åˆ†ï¼Œmaskä¹‹åçš„è¾“å‡ºéœ€è¦ä¸ºè´Ÿæ— ç©·**ï¼Œè¿™æ ·softmaxä¹‹åè¾“å‡ºæ‰ä¸º0ã€‚

#### ä¸ºä»€ä¹ˆè¦multi-head

æ¯ä¸ªå¤´å¯ä»¥å­¦ä¹ åºåˆ—ä¸­ä¸åŒçš„å…³ç³»æ¨¡å¼ï¼Œä»å¤šä¸ªè§†è§’ç†è§£è¾“å…¥çš„æ•°æ®ã€‚æŠŠæ¯ä¸ªtokenå‘é‡é™ç»´åˆ°å¤šä¸ªå­ç©ºé—´ã€‚

### Flash Attention

![img](https://wdndev.github.io/llm_interview_note/02.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84/1.attention/image/image_JdHeyN9KuN.png)

FlashAttentionçš„ä¸»è¦åŠ¨æœºå°±æ˜¯**å¸Œæœ›æŠŠSRAMåˆ©ç”¨èµ·æ¥**ï¼Œä½†æ˜¯éš¾ç‚¹å°±åœ¨äºSRAMå¤ªå°äº†ï¼Œä¸€ä¸ªæ™®é€šçš„çŸ©é˜µä¹˜æ³•éƒ½æ”¾ä¸ä¸‹å»ã€‚FlashAttentionçš„è§£å†³æ€è·¯å°±æ˜¯å°†è®¡ç®—æ¨¡å—è¿›è¡Œåˆ†è§£ï¼Œæ‹†æˆä¸€ä¸ªä¸ªå°çš„è®¡ç®—ä»»åŠ¡ã€‚

#### Softmax Tiling

##### **ï¼ˆ1ï¼‰æ•°å€¼ç¨³å®š**

SoftmaxåŒ…å«æŒ‡æ•°å‡½æ•°ï¼Œæ‰€ä»¥ä¸ºäº†é¿å…æ•°å€¼æº¢å‡ºé—®é¢˜ï¼Œå¯ä»¥å°†æ¯ä¸ªå…ƒç´ éƒ½å‡å»æœ€å¤§å€¼ï¼Œå†ç»è¿‡e^{}ï¼Œæœ€åè®¡ç®—ç»“æœå’ŒåŸæ¥çš„Softmaxæ˜¯ä¸€è‡´çš„ã€‚

##### (2) åˆ†å—è®¡ç®—Softmax

Softmaxæ˜¯æŒ‰æ¯ä¸€ä¸ªQueryå»ç®—çš„ã€‚**å°†è¾“å…¥åˆ†å‰²æˆå—ï¼Œå¹¶åœ¨è¾“å…¥å—ä¸Šè¿›è¡Œå¤šæ¬¡ä¼ é€’ï¼Œä»è€Œä»¥å¢é‡æ–¹å¼æ‰§è¡Œsoftmaxç¼©å‡**ã€‚

- ä¸æ˜¾ç¤ºæ„é€ QT^KçŸ©é˜µ
  $$
  \mathrm{softmax}(S)V=\frac{e^S V}{\sum e^S}
  $$

  ```python
  # init 
  max_score, sum_exp, acc = -inf, 0, 0
  # æ¯ä¸€ä¸ªkey tile:
  s = q*k
  new_max  = max(max_score, max(s))
  sum_exp = e^{max_score - new_max} * sum_exp + \sum{e^{s-new_max}}
  acc = e^{max_score - new_max} * acc + \sum{e^{s-new_max}} * v
  out = acc / sum_exp
  ```

- 

![img](https://wdndev.github.io/llm_interview_note/02.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84/1.attention/image/image_bck1Jw3P5A.png)

### Batch Norm & Layer Norm

ä¸ºä»€ä¹ˆTransformerç”¨Layer Normä¸ç”¨Batch Norm

BatchNormè·¨batchè®¡ç®—å‡å€¼æ–¹å·®ä¼šå—åˆ°paddingçš„å½±å“ã€‚

æŠŠæ¯ä¸€ä¸ªTokençš„å‘é‡å½’ä¸€åŒ–

#### Post LN & Pre LN

pre/post æ˜¯æŒ‡åœ¨FFNå’ŒMHAä¹‹å‰/ä¹‹å Norm

Pre Normå¾€å¾€æ›´å®¹æ˜“è®­ç»ƒï¼Œä½†æœ€ç»ˆæ•ˆæœä¸å¦‚Post Norm

#### RMSNorm

$$
y=\frac{x}{\sqrt{\frac{1}{n}\sum_{i=1}^{n}x_i^2+\epsilon}}
$$

å‡æ–¹æ ¹å±‚å½’ä¸€åŒ–

![img](https://pic3.zhimg.com/v2-29ea6a9efb8bd55687447eb9441a834a_1440w.jpg)

### ä½ç½®ç¼–ç 

#### 1.1 ç»å¯¹ä½ç½®ç¼–ç 

åœ¨ç¬¬$k$ä¸ªå‘é‡ä¸­åŠ å…¥ä½ç½®å‘é‡$p_k$,$p_k$åªä¾èµ–äºä½ç½®ä¿¡å·$k$

- è®­ç»ƒå¼ï¼šç›´æ¥**å°†ä½ç½®ç¼–ç å½“ä½œå¯è®­ç»ƒå‚æ•°**ï¼Œæ¯”å¦‚æœ€å¤§é•¿åº¦ä¸º512ï¼Œç¼–ç ç»´åº¦ä¸º768ï¼Œé‚£ä¹ˆå°±åˆå§‹åŒ–ä¸€ä¸ª512Ã—768çš„çŸ©é˜µä½œä¸ºä½ç½®å‘é‡ï¼Œè®©å®ƒéšç€è®­ç»ƒè¿‡ç¨‹æ›´æ–°ã€‚ç¼ºç‚¹æ˜¯æ²¡æœ‰**å¤–æ¨æ€§**ã€‚ï¼ˆBERTï¼‰

- ä¸‰è§’å¼ï¼š
  $$
  \begin{cases}
  \mathbf{p}_{k,2i}=\sin(k/10000^{2i/d})\\
  \mathbf{p}_{k,2i+1}=\cos(k/10000^{2i/d})\\
  \end{cases}
  $$
  

#### 1.2 ç›¸å¯¹ä½ç½®ç¼–ç 

åœ¨**ç®—Attentionçš„æ—¶å€™è€ƒè™‘å½“å‰ä½ç½®ä¸è¢«Attentionçš„ä½ç½®çš„ç›¸å¯¹è·ç¦»**ã€‚Rope.

å‡è®¾è®¡ç®—må’Œnçš„attention
$$
a_{m,n}=\frac{\exp{(\frac{x_m^Tk_n}{\sqrt{d_k}})}}{\sum_{j=1}^{N} \exp{(\frac{q_m^Tk_j}{\sqrt{d_k}})}}
$$
RoPE æ—¨åœ¨é€šè¿‡ä¸€ç§æ–°çš„æ–¹å¼å°† **ç›¸å¯¹ä½ç½®ä¿¡æ¯** ç›´æ¥åµŒå…¥åˆ° **è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ Query å’Œ Key** ä¸­ï¼Œå®ç°å¯¹ **ä»»æ„é•¿åºåˆ—çš„å¼ºæ³›åŒ–èƒ½åŠ›**ã€‚
$$
\mathrm{score}_{i,j}^{rope}=(\mathbf{q}_i \cdot R(i))^T(\mathbf{k}_j \cdot R(j))
$$
Ropeä¼šå¯¹å‘é‡çš„æ¯ä¸¤ä¸ªç»´åº¦è¿›è¡Œä¸€æ¬¡æ—‹è½¬
$$
\mathbf{x}=[(x_0,x_1),\ldots ,(x_{d-2},x_{d-1})]
$$
å‰é¢çš„ç»´åº¦æ—‹è½¬çš„å¤šï¼Œåé¢æ—‹è½¬çš„å°‘

![](https://shkaaa.github.io/picx-images-hosting/Screenshot-2025-05-06-at-10.37.45.3rbfx2zyen.webp)

![](https://shkaaa.github.io/picx-images-hosting/Screenshot-2025-05-06-at-12.16.56.86tv2fvwip.webp)

```python
import torch
import math

def apply_rope(x, seq_len, head_dim):
  # torch.arange(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)
    position_ids = torch.arange(seq_len, dtype=torch.float).unsqueeze(1)
    dim_ids = torch.arange(0, head_dim, 2, dtype=torch.float)
    inv_freq = 1.0 / (10000 ** (dim_ids / head_dim))
    sinusoid_inp = torch.einsum("i,j->ij", position_ids, inv_freq)
    sin = torch.sin(sinusoid_inp)
    cos = torch.cos(sinusoid_inp)

    x1 = x[..., ::2]
    x2 = x[..., 1::2]
    x_rotated = torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)
    return x_rotated
```

| **æ“ä½œç±»å‹** | **einsum å†™æ³•** | **ç­‰ä»· PyTorch æ“ä½œ**           |
| ------------ | --------------- | ------------------------------- |
| å‘é‡ç‚¹ç§¯     | "i,i->"         | torch.dot(a, b)                 |
| çŸ©é˜µä¹˜æ³•     | "ik,kj->ij"     | torch.matmul(A, B)              |
| å¤–ç§¯         | "i,j->ij"       | a.unsqueeze(1) * b.unsqueeze(0) |
| è½¬ç½®         | "ij->ji"        | a.T                             |
| æ‰¹æ¬¡çŸ©é˜µä¹˜æ³• | "bij,bjk->bik"  | torch.bmm(A, B)                 |
| å¹¿æ’­ä¹˜æ³•     | "bcd,cd->bcd"   | a * bï¼ˆå¸¦å¹¿æ’­çš„ elementwiseï¼‰   |
| æ€»å’Œç¼©å‡     | "abc->"         | a.sum()                         |

## Tokenize åˆ†è¯

- **word/è¯**ï¼Œè¯ï¼Œæ˜¯æœ€è‡ªç„¶çš„è¯­è¨€å•å…ƒã€‚å¯¹äºè‹±æ–‡ç­‰è‡ªç„¶è¯­è¨€æ¥è¯´ï¼Œå­˜åœ¨ç€å¤©ç„¶çš„åˆ†éš”ç¬¦ï¼Œå¦‚ç©ºæ ¼æˆ–ä¸€äº›æ ‡ç‚¹ç¬¦å·ç­‰ï¼Œå¯¹è¯çš„åˆ‡åˆ†ç›¸å¯¹å®¹æ˜“ã€‚ä½†æ˜¯å¯¹äºä¸€äº›ä¸œäºšæ–‡å­—åŒ…æ‹¬ä¸­æ–‡æ¥è¯´ï¼Œå°±éœ€è¦æŸç§åˆ†è¯ç®—æ³•æ‰è¡Œã€‚é¡ºä¾¿è¯´ä¸€ä¸‹ï¼ŒTokenizersåº“ä¸­ï¼ŒåŸºäºè§„åˆ™åˆ‡åˆ†éƒ¨åˆ†ï¼Œ**é‡‡ç”¨äº†spaCyå’ŒMosesä¸¤ä¸ªåº“**ã€‚å¦‚æœåŸºäºè¯æ¥åšè¯æ±‡è¡¨ï¼Œç”±äº**é•¿å°¾ç°è±¡**çš„å­˜åœ¨ï¼Œ**è¿™ä¸ªè¯æ±‡è¡¨å¯èƒ½ä¼šè¶…å¤§**ã€‚åƒTransformer XLåº“å°±ç”¨åˆ°äº†ä¸€ä¸ª**26.7ä¸‡**ä¸ªå•è¯çš„è¯æ±‡è¡¨ã€‚è¿™éœ€è¦æå¤§çš„embedding matrixæ‰èƒ½å­˜å¾—ä¸‹ã€‚embedding matrixæ˜¯ç”¨äºæŸ¥æ‰¾å–ç”¨tokençš„embedding vectorçš„ã€‚è¿™å¯¹äºå†…å­˜æˆ–è€…æ˜¾å­˜éƒ½æ˜¯æå¤§çš„æŒ‘æˆ˜ã€‚å¸¸è§„çš„è¯æ±‡è¡¨ï¼Œ**ä¸€èˆ¬å¤§å°ä¸è¶…è¿‡5ä¸‡**ã€‚
- **char/å­—ç¬¦**ï¼Œå³æœ€åŸºæœ¬çš„å­—ç¬¦ï¼Œå¦‚è‹±è¯­ä¸­çš„'a','b','c'æˆ–ä¸­æ–‡ä¸­çš„'ä½ 'ï¼Œ'æˆ‘'ï¼Œ'ä»–'ç­‰ã€‚è€Œä¸€èˆ¬æ¥è®²ï¼Œå­—ç¬¦çš„æ•°é‡æ˜¯**å°‘é‡æœ‰é™**çš„ã€‚è¿™æ ·åšçš„é—®é¢˜æ˜¯ï¼Œç”±äºå­—ç¬¦æ•°é‡å¤ªå°ï¼Œæˆ‘ä»¬åœ¨ä¸ºæ¯ä¸ªå­—ç¬¦å­¦ä¹ åµŒå…¥å‘é‡çš„æ—¶å€™ï¼Œæ¯ä¸ªå‘é‡å°±å®¹çº³äº†å¤ªå¤šçš„è¯­ä¹‰åœ¨å†…ï¼Œå­¦ä¹ èµ·æ¥éå¸¸å›°éš¾ã€‚
- **subword/å­è¯çº§**ï¼Œå®ƒä»‹äºå­—ç¬¦å’Œå•è¯ä¹‹é—´ã€‚æ¯”å¦‚è¯´'Transformers'å¯èƒ½ä¼šè¢«åˆ†æˆ'Transform'å’Œ'ers'ä¸¤ä¸ªéƒ¨åˆ†ã€‚è¿™ä¸ªæ–¹æ¡ˆ**å¹³è¡¡äº†è¯æ±‡é‡å’Œè¯­ä¹‰ç‹¬ç«‹æ€§**ï¼Œæ˜¯ç›¸å¯¹è¾ƒä¼˜çš„æ–¹æ¡ˆã€‚å®ƒçš„å¤„ç†åŸåˆ™æ˜¯ï¼Œ**å¸¸ç”¨è¯åº”è¯¥ä¿æŒåŸçŠ¶ï¼Œç”Ÿåƒ»è¯åº”è¯¥æ‹†åˆ†æˆå­è¯ä»¥å…±äº«tokenå‹ç¼©ç©ºé—´**ã€‚

### BPE(Byte-Pair Encoding)

Qwenè‹±æ–‡ -- BPEï¼›å¯¹äº **ä¸­æ–‡**å­—ç¬¦ï¼š**æ¯ä¸ªæ±‰å­—å½“ä½œä¸€ä¸ªç‹¬ç«‹çš„ token ç¼–ç **ï¼ˆä¸åƒä¼ ç»Ÿ BPE éœ€è¦æ‹¼æ¥å¤šä¸ªå­—ç¬¦ï¼‰ã€‚

BPEï¼Œå³å­—èŠ‚å¯¹ç¼–ç ã€‚å…¶æ ¸å¿ƒæ€æƒ³åœ¨äºå°†**æœ€å¸¸å‡ºç°çš„å­è¯å¯¹åˆå¹¶ï¼Œç›´åˆ°è¯æ±‡è¡¨è¾¾åˆ°é¢„å®šçš„å¤§å°æ—¶åœæ­¢**ã€‚ä»å‰åˆ°åéå†ç›¸é‚»çš„pairï¼Œæ‰¾é¢‘æ•°æœ€å¤§çš„åˆå¹¶ã€‚

### WordPiece

Bertæ‰€ç”¨**å­è¯çº§åˆ«çš„åˆ†è¯ç®—æ³•**ã€‚

> å“ªä¸€å¯¹ç»„åˆåçš„ token åºåˆ—åœ¨é¢„æµ‹ç›®æ ‡è¯ï¼ˆå¦‚ä¸‹ä¸€è¯ï¼‰æ—¶ä½¿æ•´ä½“è¯­è¨€æ¨¡å‹çš„æ¦‚ç‡æœ€å¤§

æœ€å¤§çš„
$$
\frac{P(AB)}{P(A)P(B)}
$$

1. ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„å­è¯ç»„åˆã€‚
2. è®¡ç®—æ¯ä¸ªå€™é€‰è¯çš„å¾—åˆ†ï¼Œé€‰æ‹©å¾—åˆ†æœ€é«˜çš„å€™é€‰è¯åŠ å…¥è¯æ±‡è¡¨ã€‚

### Unigram

æ¯æ¬¡ä»è¯æ±‡è¡¨ä¸­åˆ é™¤è¯æ±‡çš„**åŸåˆ™æ˜¯ä½¿é¢„å®šä¹‰çš„æŸå¤±æœ€å°**

Unigramç®—æ³•æ¯æ¬¡**ä¼šä»è¯æ±‡è¡¨ä¸­æŒ‘å‡ºä½¿å¾—losså¢é•¿æœ€å°çš„10%~20%çš„è¯æ±‡**æ¥åˆ é™¤ã€‚

#### Token é‡å¤

**å¢å¼ºå¸¸è§æ¨¡å¼çš„å­¦ä¹ **ï¼Œç¼“è§£ç¨€ç–æ€§ï¼Œæé«˜ç‰¹å®šé¢†åŸŸè¡¨ç°

è¿‡æ‹Ÿåˆï¼Œé™ä½å¤šæ ·æ€§ä¸åˆ›é€ æ€§ï¼Œæµªè´¹è®­ç»ƒèµ„æº

## æ¿€æ´»å‡½æ•°

### FFNå±‚çš„è®¡ç®—å…¬å¼

$$
\mathrm{FFN} = \mathrm{Relu}(\mathbf{xW}_1+\mathbf{b}_1)\mathbf{W}_2+\mathbf{b}_2
$$

#### Gelu

$$
\text{GELU}(x) = x \cdot \Phi(x)
$$

#### Geluè¿‘ä¼¼

$$
0.5x \left(1 + \tanh\left( \sqrt{\frac{2}{\pi}} \left(x + 0.044715x^3 \right) \right) \right)
$$



### SwiGLU

SwiGLU æ˜¯ä¸€ç§**é—¨æ§æ¿€æ´»æœºåˆ¶** ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ ReLUã€GELUï¼‰ï¼Œå®ƒèƒ½æä¾›æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›å’Œæ›´ä¼˜çš„è®­ç»ƒæ•ˆæœã€‚å…¶å…¬å¼å¦‚ä¸‹
$$
\text{SwiGLU}(x) = \text{GELU}(xW_g + b_g) \otimes (xW_x + b_x)
$$

$$
\text{SwiGLU}(x) = \text{SiLU}(xW_g + b_g) \otimes (xW_x + b_x)
$$

### SiLU vs ReLU vs GELU vs SwiGLU

| æ¿€æ´»å‡½æ•°     | å…¬å¼              | æ˜¯å¦å¹³æ»‘ | æ˜¯å¦æœ‰é—¨æ§æœºåˆ¶ | æ˜¯å¦å¸¸ç”¨åœ¨å¤§æ¨¡å‹ä¸­                  |
| ------------ | ----------------- | -------- | -------------- | ----------------------------------- |
| ReLU         | max(0,*x*)        | âŒ ä¸å…‰æ»‘ | âŒ æ—            | âŒ å·²è¾ƒå°‘ä½¿ç”¨                        |
| GELU         | *x*â‹…Î¦(*x*)        | âœ… æ˜¯     | âŒ æ—            | âœ… æå…¶å¹¿æ³›                          |
| SiLU / Swish | *x*â‹…*Ïƒ*(*x*)      | âœ… æ˜¯     | âœ… æœ‰           | âœ… å°æ¨¡å‹/é«˜æ•ˆæ¨¡å‹ä¸­å¸¸ç”¨             |
| SwiGLU       | GELU(xW_g)âŠ—(xW_x) | âœ… æ˜¯     | âœ… æœ‰           | âœ… å½“å‰ä¸»æµï¼ˆLLaMA/Qwen/Mistral ç­‰ï¼‰ |

GLU -- é—¨æ§æœºåˆ¶ Swish -- æ¿€æ´»å‡½æ•°
$$
\mathrm{Swish}(x)=x\frac{1}{1+e^{-x}}
$$

## è§£ç 

å‚æ•°

```json
{
 "top_k": 10,
 "temperature": 0.95,
 "num_beams": 1,
 "top_p": 0.8,
 "repetition_penalty": 1.5,
 "max_tokens": 30000,
 "message": [
        {
 "content": "ä½ å¥½ï¼",
 "role": "user"
        }
    ]
}

```

#### TOP-Ké‡‡æ ·

é‡‡æ ·æ¦‚ç‡æœ€é«˜çš„top-kä¸ªå€¼

#### TOP-Pé‡‡æ ·

ç´¯è®¡æ¦‚ç‡è¾¾åˆ°é˜ˆå€¼

#### Temperature

ä½œç”¨äº Softmaxï¼Œè°ƒæ•´æ¦‚ç‡åˆ†å¸ƒ
$$
\mathrm{Softmax}(z_i)=\frac{\exp(\frac{z_i}{T})}{\sum_j\exp(\frac{z_i}{T})}
$$

#### è”åˆé‡‡æ ·

é€šå¸¸æ˜¯å°† **top-kã€top-pã€Temperature è”åˆèµ·æ¥ä½¿ç”¨**ã€‚ä½¿ç”¨çš„å…ˆåé¡ºåºæ˜¯` top-k->top-p->Temperature`ã€‚

è¿˜æ˜¯ä»¥å‰é¢çš„ä¾‹å­ä¸ºä¾‹ã€‚

é¦–å…ˆè®¾ç½® `top-k = 3`ï¼Œè¡¨ç¤ºä¿ç•™æ¦‚ç‡æœ€é«˜çš„3ä¸ª tokenã€‚è¿™æ ·å°±ä¼šä¿ç•™å¥³å­©ã€é‹å­ã€å¤§è±¡è¿™3ä¸ª tokenã€‚

- å¥³å­©ï¼š0.664
- é‹å­ï¼š0.199
- å¤§è±¡ï¼š0.105

æ¥ä¸‹æ¥ï¼Œå¯ä»¥ä½¿ç”¨ top-p çš„æ–¹æ³•ï¼Œä¿ç•™æ¦‚ç‡çš„ç´¯è®¡å’Œè¾¾åˆ° 0.8 çš„å•è¯ï¼Œä¹Ÿå°±æ˜¯é€‰å–å¥³å­©å’Œé‹å­è¿™ä¸¤ä¸ª tokenã€‚æ¥ç€ä½¿ç”¨ Temperature = 0.7 è¿›è¡Œå½’ä¸€åŒ–ï¼Œå˜æˆï¼š

- å¥³å­©ï¼š0.660
- é‹å­ï¼š0.340

## MOEæ¨¡å‹

- å¯¹äºä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œ**å¯¹æ¯ä¸€ä¸ªæ ·æœ¬éƒ½ä¼šæ¿€æ´»æ•´ä¸ªæ¨¡å‹**ï¼Œè¿™ä¼šå¯¼è‡´åœ¨è®­ç»ƒæˆæœ¬ä¸Šï¼Œä»¥**å¤§çº¦äºŒæ¬¡æ–¹çš„é€Ÿåº¦å¢é•¿**ï¼Œå› ä¸º**æ¨¡å‹å¤§å°å’Œè®­ç»ƒæ ·æœ¬æ•°ç›®éƒ½å¢åŠ äº†**ã€‚

å¯¹å°æ¨¡å‹å¢åŠ ä¸“å®¶ï¼Ÿ

