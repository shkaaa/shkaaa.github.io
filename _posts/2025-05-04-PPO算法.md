---
layout:     post
title:     PPO/GRPO算法
subtitle:   LLM 
date:       2025-05-04
author:     SHK
header-img: img/post-bg-debug.png
catalog: true
tags: 
    - Reinforcement Learning
---

- 策略模型：待优化的模型，参与参数更新
  - 策略损失
- 价值模型：计算当前动作和状态的期望回报，可由奖励模型或策略模型初始化而来，参与参数更新
  - 价值损失
- 奖励模型：计算当前动作的基石奖励，不参与参数更新
- 参考模型：原始模型，不参与参数更新

Action: 下一个token

Reward只针对完整输出给出一个得分（Score）

每个token奖惩项：KL散度*-0.2

![](https://shkaaa.github.io/picx-images-hosting/Screenshot-2025-05-04-at-13.32.47.ma95oh6l.webp)

![](https://shkaaa.github.io/picx-images-hosting/Screenshot-2025-05-04-at-14.38.39.7snf8uu4we.webp)

![](https://shkaaa.github.io/picx-images-hosting/Screenshot-2025-05-04-at-14.43.02.icbxt86va.webp)

![](https://shkaaa.github.io/picx-images-hosting/Screenshot-2025-05-04-at-14.52.44.6bha7484oq.webp)



# PPO/GRPO强化学习

**目标**：训练一个Policy神经网络$\pi$,在所有状态S下，给出相应Action，得到Return的期望最大

**目标**：训练一个Policy神经网络$\pi$,在所有的Trajectory下，得到Return的期望最大

$\tau$代表一个Trajectory，概率分布取决于参数为$\theta$的神经网络
$$
E(R(\tau))_{\tau \sim P_\theta(\tau)} = \sum_{\tau} R(\tau) P_\theta(\tau)
$$

$$
\nabla E(R(\tau))_{\tau \sim P_\theta(\tau)}&=\nabla\sum_{\tau} R(\tau) P_\theta(\tau)\\
&=\sum_{\tau} R(\tau) \nabla P_\theta(\tau)&\\
&=\sum_{\tau} R(\tau) \nabla P_\theta(\tau)\frac{P_\theta(\tau)}{P_\theta(\tau)}\\
&=\sum_{\tau}   P_\theta(\tau) R(\tau)\frac{\nabla P_\theta(\tau)}{P_\theta(\tau)}\\
&\approx \frac{1}{N}\sum_{n=1}^N R(\tau^n) \frac{\nabla P_\theta(\tau)}{P_\theta(\tau)}\\
&=\frac{1}{N}\sum_{n=1}^N R(\tau^n) \nabla\log{P_\theta(\tau^n)}\\
&=\frac{1}{N}\sum_{n=1}^N R(\tau^n) \nabla\log{\prod_{t=1}^{T_n}P_\theta(a_{n}^{t}|s_{n}^{t})}\\
&=\frac{1}{N}\sum_{n=1}^N \sum_{t=1}^{T_n} R(\tau^n) \nabla \log P_\theta(a_{n}^{t}|s_{n}^{t})
$$

梯度策略
$$
\mathrm{Loss}=-\frac{1}{N}\sum_{n=1}^N \sum_{t=1}^{T_n} R(\tau^n) \log P_\theta(a_{n}^{t}|s_{n}^{t})
$$

#### 例子：

输入：当前游戏画面state -> CNN -> softmax 三个动作的概率

连续玩N场游戏，得到N个Trajectory 和Return值

#### On Policy

运行模型 -- 采集数据 -- 训练模型 -- 采集数据

缺点：浪费时间，大部分时间在采集数据

#### 一个动作只能影响之后的Reward，影响可能会逐渐衰减

$$
R(\tau^n) \rightarrow \sum_{t'=t}^{T_n}\gamma^{t'-t}r_{t'}^{n}=R_t^n
$$

#### 相对好的动作概率增加，相对坏的动作概率减小。需要一个Baseline

$$
\mathrm{Loss}=-\frac{1}{N}\sum_{n=1}^N \sum_{t=1}^{T_n} (R_t^n-B(s_n^t)) \log P_\theta(a_{n}^{t}|s_{n}^{t})
$$

#### Action-Value Function

$R_t^n$每一次都是随机采样，方差很大，训练不稳定

$Q_\theta(s,a)$是在state s 下，做出Action a，期望的回报。动作价值函数

#### State-Value Function

$V_\theta(s)$,在一个state下期望的回报。状态价值函数

#### Advantage Function

$A_\theta(s,a)=Q_\theta(s,a)-V_\theta(s)$ 优势函数
$$
\mathrm{Loss}=-\frac{1}{N}\sum_{n=1}^N \sum_{t=1}^{T_n} A_\theta(a_{n}^{t}|s_{n}^{t}) \log P_\theta(a_{n}^{t}|s_{n}^{t})
$$

$$
Q_\theta(s_t,a)=r_t+\gamma \times V_\theta(s_{t+1})\\
A_\theta(s_t,a)=r_t+\gamma \times V_\theta(s_{t+1})-V_\theta(s_t)\\
V_\theta(s_{t+1}) \approx r_{t+1}+\gamma\times V_{\theta}(s_{t+2})
$$

所以只需要训练一个$V_{\theta}$

A可以采样多步，采样的步数越多，偏差越小，方差越大

定义
$$
\delta_t^V=r_t+\gamma *V_{\theta}(s_{t+1})-V_{\theta}(s_t)\\
s.t. A_{\theta}^5(s_t,a)= \delta_t^V+\gamma\delta_{t+1}^V+\gamma^2\delta_{t+2}^V+\cdots
$$

#### GAE:

给优势函数每一步采样都分配不同的权重，后加和。

![https://shkaaa.github.io/picx-images-hosting/Screenshot-2025-05-04-at-13.09.27.7pi4kgm6b.webp](https://shkaaa.github.io/picx-images-hosting/Screenshot-2025-05-04-at-13.09.27.7pi4kgm6b.webp)

![](https://shkaaa.github.io/picx-images-hosting/Screenshot-2025-05-04-at-13.10.53.3nrtwns458.webp)

![](https://shkaaa.github.io/picx-images-hosting/Screenshot-2025-05-04-at-13.12.45.2dowqccixu.webp)

<img src="https://shkaaa.github.io/picx-images-hosting/Screenshot-2025-05-04-at-13.15.41.7axdk6u2e7.webp" style="zoom:67%;" />
![](https://shkaaa.github.io/picx-images-hosting/Screenshot-2025-05-04-at-13.17.32.1zigzhad4x.webp)
![](https://shkaaa.github.io/picx-images-hosting/Screenshot-2025-05-04-at-13.19.02.491hiyx1hs.webp)









