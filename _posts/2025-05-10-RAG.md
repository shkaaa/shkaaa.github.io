---
layout:     post
title:     RAG & Agent
subtitle:   LLM
date:       2025-05-10
author:     SHK
header-img: img/post-bg-debug.png
catalog: true
tags: 
    - NLP
---

# RAG & Agent & LangChain

## RAG的核心流程

检索器Retriever + 生成器Generator

### 构造增强Prompt

根据以下信息回答问题：

[检索到的段落1]  
[检索到的段落2]  
...

问题：{用户的问题}

### 模版格式化Prompt

以下是与问题相关的信息：

{retrieved_context}

请根据以上信息回答以下问题：

{query}

### 多段检索内容合并策略

当检索到多个相关信息片段时，有几种处理方式：

- **直接拼接** ：将所有段落按相关性排序后依次拼接到 prompt 中。
- **摘要合并** ：先对多个段落做简要总结，再输入给模型。
- **加权选择** ：只保留最相关的前 N 个段落，避免 prompt 过长影响性能。

### 解决的问题

- 长尾知识
- 私有数据
- 来源验证/可解释性
- 数据新鲜度

## 数据和索引模块

#### 数据获取

数据获取模块的作用一般是**将多种来源、多种类型和格式的外部数据转换成一个统一的文档对象** ( Document Object )。文档对象除了包含原始的文本内容，一般还会携带文档的**元信息 ( Metadata )\**，\**可以用于后期的检索和过滤**。元信息包括但不限于：

#### 文本分块

Langchain.text_splitter

#### 数据索引 （流行使用向量索引）

利用**文本嵌入模型** ( Text Embedding Model ) 将文本块映射成一个固定长度的向量，然后存储在**向量数据库**中。

- 文本潜入模型：
  - Sentence Transformers -- 基于孪生bert
  - openai：text-embedding-ada-002
  - Instructor模型，经过指令微调的文本嵌入模型
- 相似向量检索
  - cosine similarity

##### 向量数据库

1. **索引**: 使用乘积量化 ( Product Quantization ) 、局部敏感哈希 ( LSH )、HNSW 等算法对向量进行索引，这一步将向量映射到一个数据结构，以实现更快的搜索。
2. **查询**: 将查询向量和索引向量进行比较，以找到最近邻的相似向量。
3. **后处理**: 有些情况下，向量数据库检索出最近邻向量后，对其进行后处理后再返回最终结果。

### 查询和检索模块

#### 同义改写

将查询语句改写为多种同义疑问句。

#### 查询分解

将查询分为多个简单的子查询。

#### HyDE （Hypothetical Document Embeddings）

给定一个初始查询，首先利用LLM生成一个假设的文档或者回复，然后以这个假设文档或者回复作为新的查询进行检索。（不靠谱，可能出现误导）

#### 排序和后处理

- 基于相似度分数进行过滤和排序
- 基于关键词进行过滤，比如限定包含或者不包含某些关键词
- 让 LLM 基于返回的相关文档及其相关性得分来重新排序
- 基于时间进行过滤和排序，比如只筛选最新的相关文档
- 基于时间对相似度进行加权，然后进行排序和筛选

## 回复生成

prompt模版

## RAG调用模式

**模式一：** 非结构化数据通过Embedding Model把非结构化数据进行embedding存到向量数据库中，然后形成Construct Prompts给到LLM。LLM返回结果给到用户。

**模式二：** 用户提出问题，下一步把问题通过Embedding Model向量化，然后保存到长时记忆数据库（向量数据库）中，然后调用LLM完成问题的回答，接下来将大模型的回答存到长时记忆数据库中，最后返回给用户。

**模式三：** 用户问问题，下一步把问题通过Embedding Model向量化，然后从Cache中（向量数据库）查询类似的问题和答案，返回给用户。如果没有命中，则去和LLM交互。然后把LLM的回答存到Cache中，最后把回答返回给用户。

## RAG vs. SFT

<img src="https://wdndev.github.io/llm_interview_note/08.%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BArag/rag%EF%BC%88%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90%EF%BC%89%E6%8A%80%E6%9C%AF/image/image_C5NZymFSB9.png" alt="img" style="zoom:50%;" />

## Agent

- AutoGPT

## LangChain

提供了多种组件，降低了大模型应用的开发难度。

LangChain 的提供了以下 6 种标准化、可扩展的接口并且可以外部集成的核心模块：

1. **模型输 入/输出（Model I/O）**：与语言模型交互的接口；
2. **数据连接（Data connection）**：与特定应用程序的数 据进行交互的接口；
3. **链（Chains）**：用于复杂的应用的调用序列；
4. **智能体（Agents）**：语言模型作为推理器决定要执行的动作序列；
5. **记忆（Memory）**：用于链的多次运行之间持久化应用程序状态；
6. **回调 （Callbacks）**：记录和流式传输任何链式组装的中间步骤。

```python
from langchain.document_loaders import DirectoryLoader 
from langchain.embeddings.openai import OpenAIEmbeddings 
from langchain.text_splitter import CharacterTextSplitter 
from langchain.vectorstores import Chroma 
from langchain.chains import ChatVectorDBChain, ConversationalRetrievalChain 
from langchain.chat_models import ChatOpenAI 
from langchain.chains import RetrievalQA 

# 从本地读取相关数据 
loader = DirectoryLoader( 
  './Langchain/KnowledgeBase/', glob='**/*.pdf', show_progress=True 
) 

docs = loader.load()

# 将文件进行切分 
text_splitter = CharacterTextSplitter( chunk_size=1000, chunk_overlap=0 ) 
docs_split = text_splitter.split_documents(docs) 

# 初始化 OpenAI Embeddings 
embeddings = OpenAIEmbeddings() 

# 将数据存入 Chroma 向量存储 
vector_store = Chroma.from_documents(docs, embeddings) 

# 初始化检索器，使用向量存储 
retriever = vector_store.as_retriever() 
system_template = """ Use the following pieces of context to answer the users question. If you don't know the answer, just say that you don't know, don't try to make up an answer. Answering these questions in Chinese. 
----------
{question}
----------
{chat_history}
"""

# 构建初始 Messages 列表 
messages = [ 
  SystemMessagePromptTemplate.from_template(system_template), 
  HumanMessagePromptTemplate.from_template('{question}') 
] 

# 初始化 Prompt 对象 
prompt = ChatPromptTemplate.from_messages(messages) 

# 初始化大语言模型，使用 OpenAI API 
llm=ChatOpenAI(temperature=0.1, max_tokens=2048) 

# 初始化问答链 
qa = ConversationalRetrievalChain.from_llm(llm,retriever,condense_question_prompt=prompt) 

chat_history = [] 
while True: 
  question = input('问题：') 
  # 开始发送问题 chat_history 为必须参数, 用于存储对话历史 
  result = qa({'question': question, 'chat_history': chat_history}) 
  chat_history.append((question, result['answer'])) 
  print(result['answer'])
```

![整体框架](https://datawhalechina.github.io/llm-universe/figures/C6-1-structure.jpg)